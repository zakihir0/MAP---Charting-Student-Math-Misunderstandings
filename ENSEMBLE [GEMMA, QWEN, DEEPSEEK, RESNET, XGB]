{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcc6aee",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.005776,
     "end_time": "2025-10-09T01:45:00.074357",
     "exception": false,
     "start_time": "2025-10-09T01:45:00.068581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble Gemma-2 9b, Qwen3 8b & Deepseek math 7b \n",
    "\n",
    "Inference runs in 2hours, so lot of room for more models. We infer Gemma 9b on 2 gpus because it is loaded in fp16. We run Qwen 3 8b and deepseek math 7b parallel on 2 gpus, to save time. After inference, for ensembling we use prob confidence, weighted average and agreement between models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Credits:   \n",
    "@cdeotte - [Gemma 9b weights](https://www.kaggle.com/datasets/cdeotte/gemma2-9b-it-cv945)  \n",
    "@jaytonde - [Qwen 3 8b weights](https://www.kaggle.com/datasets/jaytonde/qwen3-8b-map-competition)  \n",
    "@jaytonde - [Deepseek math 7b weights](https://www.kaggle.com/datasets/jaytonde/deekseepmath-7b-map-competition) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4745e711",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T01:45:00.085356Z",
     "iopub.status.busy": "2025-10-09T01:45:00.085071Z",
     "iopub.status.idle": "2025-10-09T01:48:38.173769Z",
     "shell.execute_reply": "2025-10-09T01:48:38.172929Z"
    },
    "papermill": {
     "duration": 218.096142,
     "end_time": "2025-10-09T01:48:38.175618",
     "exception": false,
     "start_time": "2025-10-09T01:45:00.079476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "ydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\r\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\r\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qq --no-index --find-links=/kaggle/input/map-library-public \\ vllm==\"0.9.2\"\n",
    "# check \n",
    "import vllm \n",
    "import transformers \n",
    "import torch \n",
    "assert vllm.__version__ == \"0.9.2\" \n",
    "assert transformers.__version__ == \"4.52.4\" \n",
    "assert torch.__version__ == \"2.7.0+cu126\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e04c64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T01:48:38.187017Z",
     "iopub.status.busy": "2025-10-09T01:48:38.186588Z",
     "iopub.status.idle": "2025-10-09T01:48:38.311639Z",
     "shell.execute_reply": "2025-10-09T01:48:38.310590Z"
    },
    "papermill": {
     "duration": 0.132534,
     "end_time": "2025-10-09T01:48:38.313214",
     "exception": false,
     "start_time": "2025-10-09T01:48:38.180680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU count: 2\n",
      "=== GPU 0 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "=== GPU 1 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"現在の GPU メモリ使用状況を表示する\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available.\")\n",
    "        return\n",
    "    \n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(\"GPU count:\", gpu_count)\n",
    "\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"=== GPU {i} ===\")\n",
    "        print(\"Name:\", torch.cuda.get_device_name(i))\n",
    "        print(\"Memory Allocated:\", round(torch.cuda.memory_allocated(i)/1024**2), \"MB\")\n",
    "        print(\"Memory Reserved: \", round(torch.cuda.memory_reserved(i)/1024**2), \"MB\")\n",
    "        print(\"Max Allocated:   \", round(torch.cuda.max_memory_allocated(i)/1024**2), \"MB\")\n",
    "        print(\"Max Reserved:    \", round(torch.cuda.max_memory_reserved(i)/1024**2), \"MB\")\n",
    "        print()\n",
    "\n",
    "# 使い方:\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5e577",
   "metadata": {
    "papermill": {
     "duration": 0.005004,
     "end_time": "2025-10-09T01:48:38.323547",
     "exception": false,
     "start_time": "2025-10-09T01:48:38.318543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model-1: Gemma2  9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f9010f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T01:48:38.334474Z",
     "iopub.status.busy": "2025-10-09T01:48:38.334171Z",
     "iopub.status.idle": "2025-10-09T01:48:38.342359Z",
     "shell.execute_reply": "2025-10-09T01:48:38.341452Z"
    },
    "papermill": {
     "duration": 0.015346,
     "end_time": "2025-10-09T01:48:38.343750",
     "exception": false,
     "start_time": "2025-10-09T01:48:38.328404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gemma2_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gemma2_inference.py\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import os\n",
    "from IPython.display import display, Math, Latex\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from peft import PeftModel\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "lora_path = \"/kaggle/input/gemma2-9b-it-cv945\"\n",
    "MAX_LEN = 256\n",
    "# helpers\n",
    "def format_input(row):\n",
    "    x = \"Yes\"\n",
    "    if not row['is_correct']:\n",
    "        x = \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct? {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\"\n",
    "    )\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "\n",
    "train.Misconception = train.Misconception.fillna('NA')\n",
    "train['target'] = train.Category+\":\"+train.Misconception\n",
    "train['label'] = le.fit_transform(train['target'])\n",
    "target_classes = le.classes_\n",
    "n_classes = len(target_classes)\n",
    "print(f\"Train shape: {train.shape} with {n_classes} target classes\")\n",
    "idx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\n",
    "correct = train.loc[idx].copy()\n",
    "correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "correct = correct.sort_values('c',ascending=False)\n",
    "correct = correct.drop_duplicates(['QuestionId'])\n",
    "correct = correct[['QuestionId','MC_Answer']]\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "# Prepare test data\n",
    "test = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "test.is_correct = test.is_correct.fillna(0)\n",
    "test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "# Create a directory for model offloading\n",
    "os.makedirs(\"offload\", exist_ok=True)\n",
    "\n",
    "# load model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"/kaggle/input/gemma2-9b-it-bf16\",\n",
    "    num_labels=n_classes,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    # offload_folder=\"offload\",  # <-- Add this line\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "model.eval()\n",
    "\n",
    "# Tokenize dataset\n",
    "ds_test = Dataset.from_pandas(test[['text']])\n",
    "ds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Create data collator for efficient batching with padding\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LEN,  \n",
    "    return_tensors=\"pt\")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ds_test,\n",
    "    batch_size=8,  \n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=True,  \n",
    "    num_workers=2     \n",
    ")\n",
    "\n",
    "# Fast inference loop\n",
    "all_logits = []\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert bfloat16 to float32 then move to CPU and store\n",
    "        all_logits.append(logits.float().cpu().numpy())\n",
    "\n",
    "# Concatenate all logits\n",
    "predictions = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "# Convert to probs\n",
    "probs = softmax(predictions, axis=1)\n",
    "\n",
    "# Get top predictions (all 65 classes ranked)\n",
    "top_indices = np.argsort(-probs, axis=1)\n",
    "\n",
    "# Decode to class names\n",
    "flat_indices = top_indices.flatten()\n",
    "decoded_labels = le.inverse_transform(flat_indices)\n",
    "top_labels = decoded_labels.reshape(top_indices.shape)\n",
    "\n",
    "# Create submission (top 3)\n",
    "joined_preds = [\" \".join(row[:3]) for row in top_labels]\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"row_id\": test.row_id.values,\n",
    "    \"Category:Misconception\": joined_preds\n",
    "})\n",
    "sub.to_csv(\"submission_gemma.csv\", index=False)\n",
    "\n",
    "prob_data = []\n",
    "for i in range(len(test)):\n",
    "    prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(25)}  # Top 25\n",
    "    prob_dict['row_id'] = test.row_id.values[i]\n",
    "    prob_dict['top_classes'] = \" \".join(top_labels[i, :25])  # Top 25 class names\n",
    "    prob_data.append(prob_dict)\n",
    "\n",
    "prob_df = pd.DataFrame(prob_data)\n",
    "prob_df.to_csv(\"submission_gemma_prob.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5644135",
   "metadata": {
    "papermill": {
     "duration": 0.005144,
     "end_time": "2025-10-09T01:48:38.354523",
     "exception": false,
     "start_time": "2025-10-09T01:48:38.349379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 2-3: Qwen 3 8b & Deepseek math 7b parallel\n",
    "Run deepseek on cuda:0 and qwen 3 on cuda:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0751490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T01:48:38.366408Z",
     "iopub.status.busy": "2025-10-09T01:48:38.365982Z",
     "iopub.status.idle": "2025-10-09T01:48:38.379894Z",
     "shell.execute_reply": "2025-10-09T01:48:38.378962Z"
    },
    "papermill": {
     "duration": 0.021723,
     "end_time": "2025-10-09T01:48:38.381286",
     "exception": false,
     "start_time": "2025-10-09T01:48:38.359563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen3_deepseek_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen3_deepseek_inference.py\n",
    "\n",
    "# we do parallel inference, for deepseek and qwen3\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "import threading\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "\n",
    "model_paths = [\n",
    "    \"/kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL\",\n",
    "   \"/kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL\"]\n",
    "\n",
    "def format_input(row):\n",
    "    x = \"This answer is correct.\"\n",
    "    if not row['is_correct']:\n",
    "        x = \"This is answer is incorrect.\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"{x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\")\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "train.Misconception  = train.Misconception.fillna('NA')\n",
    "train['target']   = train.Category + ':' +train.Misconception\n",
    "train['label']    = le.fit_transform(train['target'])\n",
    "\n",
    "n_classes = len(le.classes_)\n",
    "print(f\"Train shape: {train.shape} with {n_classes} target classes\")\n",
    "idx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\n",
    "correct = train.loc[idx].copy()\n",
    "correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "correct = correct.sort_values('c',ascending=False)\n",
    "correct = correct.drop_duplicates(['QuestionId'])\n",
    "correct = correct[['QuestionId','MC_Answer']]\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "test.is_correct = test.is_correct.fillna(0)\n",
    "test['text'] = test.apply(format_input,axis=1)\n",
    "ds_test = Dataset.from_pandas(test)\n",
    "\n",
    "\n",
    "def run_inference_on_gpu(model_path, gpu_id, test_data, output_name):\n",
    "    \"\"\"Run inference for one model on one GPU\"\"\"\n",
    "    \n",
    "    device = f\"cuda:{gpu_id}\"\n",
    "    print(f\"Loading {output_name} on {device}...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, \n",
    "        device_map=device, \n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize function\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch[\"text\"], \n",
    "                        truncation=True,\n",
    "                        max_length=256)\n",
    "    \n",
    "    ds_test = Dataset.from_pandas(test_data[['text']])\n",
    "    ds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        ds_test,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "        pin_memory=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Inference\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"{output_name}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            all_logits.append(outputs.logits.float().cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(all_logits, axis=0)\n",
    "    \n",
    "    # Process results\n",
    "    probs = softmax(predictions, axis=1)\n",
    "    top_indices = np.argsort(-probs, axis=1)\n",
    "    \n",
    "    # Decode labels\n",
    "    flat_indices = top_indices.flatten()\n",
    "    decoded_labels = le.inverse_transform(flat_indices)\n",
    "    top_labels = decoded_labels.reshape(top_indices.shape)\n",
    "    \n",
    "    # Save top-3 submission\n",
    "    joined_preds = [\" \".join(row[:3]) for row in top_labels]\n",
    "    sub = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.values,\n",
    "        \"Category:Misconception\": joined_preds\n",
    "    })\n",
    "    sub.to_csv(f\"submission_{output_name}.csv\", index=False)\n",
    "    \n",
    "    # Save probabilities for ensemble\n",
    "    prob_data = []\n",
    "    for i in range(len(predictions)):\n",
    "        prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(25)}\n",
    "        prob_dict['row_id'] = test_data.row_id.values[i]\n",
    "        prob_dict['top_classes'] = \" \".join(top_labels[i, :25])\n",
    "        prob_data.append(prob_dict)\n",
    "    \n",
    "    prob_df = pd.DataFrame(prob_data)\n",
    "    prob_df.to_csv(f\"submission_{output_name}_probabilities.csv\", index=False)\n",
    "    \n",
    "    print(f\" {output_name} completed - saved submission and probabilities\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\" Starting multi-GPU inference...\")\n",
    "start_time = time.time()\n",
    "\n",
    "threads = []\n",
    "gpu_assignments = [\n",
    "    (model_paths[0], 0, \"deepseek\"),\n",
    "    (model_paths[1], 1, \"qwen3\"),\n",
    "]\n",
    "\n",
    "# Start threads\n",
    "for model_path, gpu_id, name in gpu_assignments:\n",
    "    if gpu_id < torch.cuda.device_count():  \n",
    "        thread = threading.Thread(\n",
    "            target=run_inference_on_gpu,\n",
    "            args=(model_path, gpu_id, test, name)\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        time.sleep(10)  # Stagger starts to avoid memory issues\n",
    "\n",
    "# Wait for completion\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\" completed in {end_time - start_time:.2f} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510b46fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T01:48:38.411046Z",
     "iopub.status.busy": "2025-10-09T01:48:38.410152Z",
     "iopub.status.idle": "2025-10-09T01:48:38.422542Z",
     "shell.execute_reply": "2025-10-09T01:48:38.421717Z"
    },
    "papermill": {
     "duration": 0.037391,
     "end_time": "2025-10-09T01:48:38.423839",
     "exception": false,
     "start_time": "2025-10-09T01:48:38.386448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vllm_qwen_sft_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vllm_qwen_sft_inference.py\n",
    "import os, json, gc, time, re, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, LogitsProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# ==============================\n",
    "# パス設定\n",
    "# ==============================\n",
    "VLLM_MODEL_DIR  = \"/kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233\"\n",
    "TRAIN_CSV = \"/kaggle/input/map-charting-student-math-misunderstandings/train.csv\"\n",
    "TEST_CSV  = \"/kaggle/input/map-charting-student-math-misunderstandings/test.csv\"\n",
    "OUT_DIR   = \"/kaggle/working\"\n",
    "\n",
    "# ==============================\n",
    "# 実行設定（安定化）\n",
    "# ==============================\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "os.environ.setdefault(\"NCCL_ASYNC_ERROR_HANDLING\", \"1\")\n",
    "os.environ.setdefault(\"NCCL_SHUTDOWN_TIMEOUT\", \"5\")\n",
    "os.environ.setdefault(\"NCCL_P2P_DISABLE\", \"1\")\n",
    "os.environ.setdefault(\"NCCL_IB_DISABLE\", \"1\")\n",
    "os.environ.setdefault(\"VLLM_WORKER_MULTIPROC_METHOD\", \"spawn\")\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0,1\")\n",
    "\n",
    "# vLLM / モデル実行パラメータ\n",
    "VLLM_TP_SIZE         = 2          # T4x2\n",
    "VLLM_DTYPE           = \"float16\"\n",
    "VLLM_MAX_LEN         = 256\n",
    "VLLM_SEED            = 42\n",
    "VLLM_LOGPROBS_K      = 20         # vLLM上限\n",
    "VLLM_MEM_UTILIZATION = 0.95       # 余裕を持たせつつ高め\n",
    "\n",
    "# 出力制御\n",
    "TOP_K_SAVE  = 25   # CSVに保存する最大候補数（vLLMはlogprobs最大20）\n",
    "TOP_K_SUB   = 3    # 提出用top-k\n",
    "\n",
    "# ==============================\n",
    "# ユーティリティ\n",
    "# ==============================\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def _sync_and_free():\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            for d in range(torch.cuda.device_count()):\n",
    "                torch.cuda.reset_peak_memory_stats(d)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()\n",
    "\n",
    "def _safe_dist_destroy():\n",
    "    \"\"\"barrierは呼ばない（ハング回避）\"\"\"\n",
    "    try:\n",
    "        import torch.distributed as dist\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            try:\n",
    "                dist.destroy_process_group()\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ==============================\n",
    "# プロンプト\n",
    "# ==============================\n",
    "PROMPT_EN = \"\"\"\\\n",
    "You are a specialist in identifying the types of misunderstandings that arise from students’ answers to math problems.\n",
    "Based on the information provided below, please determine what kind of misunderstanding the student has.\n",
    "\n",
    "Question: {QuestionText}\n",
    "Answer: {MC_Answer}\n",
    "Correct: {Correct}\n",
    "Student Explanation: {StudentExplanation}\n",
    "\n",
    "Respond with ONLY the label text exactly as in the training set (e.g., \"True_Correct:NA\", \"False_Neither:NA\"). Do not add explanations.\n",
    "\"\"\"\n",
    "\n",
    "def _fmt(row) -> str:\n",
    "    return PROMPT_EN.format(\n",
    "        QuestionText=row[\"QuestionText\"],\n",
    "        MC_Answer=row[\"MC_Answer\"],\n",
    "        Correct=\"Yes\" if row[\"is_correct\"] else \"No\",\n",
    "        StudentExplanation=row[\"StudentExplanation\"],\n",
    "    ) + \" \"  # 末尾スペースで即EOS回避の保険\n",
    "\n",
    "# ===== ラベル正規化/照合 =====\n",
    "DIGIT_PREFIX_RE = re.compile(r\"^\\s*([0-9]{1,4})\")\n",
    "IM_SPECIAL = re.compile(r\"<\\|im_[^|]*\\|>\")  # <|im_end|> など\n",
    "\n",
    "def _norm_label(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip(\" .,:;\")\n",
    "\n",
    "def _build_label_index(all_completions):\n",
    "    idx = {}\n",
    "    for lab in all_completions:\n",
    "        idx[_norm_label(lab)] = lab\n",
    "        idx[_norm_label(lab.replace(\"_\", \" \"))] = lab\n",
    "        idx[_norm_label(lab.replace(\" \", \"_\"))] = lab\n",
    "    return idx\n",
    "\n",
    "def _pick_label_from_text(text: str, label_index: dict):\n",
    "    cand = _norm_label(text)\n",
    "    if not cand:\n",
    "        return None\n",
    "    if cand in label_index:\n",
    "        return label_index[cand]\n",
    "    # 先頭一致（例: \"True_Correct:NA<eos>\"）\n",
    "    for k in label_index.keys():\n",
    "        if cand.startswith(k):\n",
    "            return label_index[k]\n",
    "    return None\n",
    "\n",
    "# ===== LogitsProcessor（Case A用）=====\n",
    "class LabelOnlyLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, allowed_token_ids):\n",
    "        self.allowed_token_ids = allowed_token_ids\n",
    "    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.Tensor:\n",
    "        mask = torch.full_like(scores, float('-inf'))\n",
    "        if scores.dim() == 1:\n",
    "            mask[self.allowed_token_ids] = 0\n",
    "        elif scores.dim() == 2:\n",
    "            mask[:, self.allowed_token_ids] = 0\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected score dimensions\")\n",
    "        return scores + mask\n",
    "\n",
    "# ==============================\n",
    "# メイン\n",
    "# ==============================\n",
    "def main():\n",
    "    seed_everything(VLLM_SEED)\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(\"[INFO] ====== Qwen-SFT vLLM Inference (Hybrid) ======\")\n",
    "\n",
    "    # --- データ準備 ---\n",
    "    train = pd.read_csv(TRAIN_CSV)\n",
    "    test  = pd.read_csv(TEST_CSV)\n",
    "    print(f\"[INFO] train={train.shape}, test={test.shape}\")\n",
    "\n",
    "    # True_* の最頻回答を is_correct=1\n",
    "    idx_true = train[\"Category\"].str.startswith(\"True\")\n",
    "    correct = train.loc[idx_true].copy()\n",
    "    correct[\"cnt\"] = correct.groupby([\"QuestionId\",\"MC_Answer\"]).MC_Answer.transform(\"count\")\n",
    "    correct = (correct.sort_values(\"cnt\", ascending=False)\n",
    "                     .drop_duplicates([\"QuestionId\"])\n",
    "                     .loc[:, [\"QuestionId\",\"MC_Answer\"]])\n",
    "    correct[\"is_correct\"] = 1\n",
    "    test = test.merge(correct, on=[\"QuestionId\",\"MC_Answer\"], how=\"left\")\n",
    "    test[\"is_correct\"] = test[\"is_correct\"].fillna(0)\n",
    "\n",
    "    prompts = test.apply(_fmt, axis=1).tolist()\n",
    "    row_ids = test.row_id.values.astype(int)\n",
    "\n",
    "    # --- ラベル候補 ---\n",
    "    tokenizer_hf = AutoTokenizer.from_pretrained(VLLM_MODEL_DIR, trust_remote_code=True, local_files_only=True)\n",
    "    with open(os.path.join(VLLM_MODEL_DIR, \"all_completions.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        all_completions = json.load(f)\n",
    "\n",
    "    # 数字ラベルが単一トークンか検査\n",
    "    single_token_ok = True\n",
    "    allowed_token_ids = []\n",
    "    for i in range(len(all_completions)):\n",
    "        ids = tokenizer_hf.encode(str(i), add_special_tokens=False)\n",
    "        if len(ids) != 1:\n",
    "            single_token_ok = False\n",
    "        else:\n",
    "            allowed_token_ids.append(ids[0])\n",
    "\n",
    "    if single_token_ok:\n",
    "        print(\"[INFO] Label indices are single-token ✅  -> Use LogitsProcessor path (Case A)\")\n",
    "    else:\n",
    "        print(\"[WARN] Some label indices are multi-token -> Fallback to text-label extraction (Case B)\")\n",
    "\n",
    "    # --- vLLM 起動 ---\n",
    "    gpu_cnt = max(1, torch.cuda.device_count())\n",
    "    tp = max(1, min(VLLM_TP_SIZE, gpu_cnt))\n",
    "    print(f\"[INFO] GPU count={gpu_cnt}, tensor_parallel_size={tp}\")\n",
    "\n",
    "    llm = LLM(\n",
    "        model=str(VLLM_MODEL_DIR),\n",
    "        tensor_parallel_size=tp,\n",
    "        dtype=VLLM_DTYPE,\n",
    "        gpu_memory_utilization=VLLM_MEM_UTILIZATION,\n",
    "        enforce_eager=True,\n",
    "        max_model_len=VLLM_MAX_LEN,\n",
    "        seed=VLLM_SEED,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    vtok = llm.llm_engine.tokenizer\n",
    "\n",
    "    prob_rows, topk_names = [], []\n",
    "\n",
    "    if single_token_ok:\n",
    "        # ===== Case A: 1トークン分類（参考コードと同じ流儀） =====\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.0, top_p=1.0, top_k=-1,\n",
    "            max_tokens=1,\n",
    "            logprobs=min(3, VLLM_LOGPROBS_K, TOP_K_SAVE),  # MAP@3 を念頭に\n",
    "            stop_token_ids=[vtok.eos_token_id],\n",
    "            logits_processors=[LabelOnlyLogitsProcessor(allowed_token_ids)],\n",
    "        )\n",
    "        print(\"[INFO] Generating (Case A)...\")\n",
    "        t0 = time.time()\n",
    "        outputs = list(llm.generate(prompts, sampling_params))\n",
    "        print(f\"[INFO] Generation done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "        for i, out in enumerate(outputs):\n",
    "            rec = {\"row_id\": int(row_ids[i])}\n",
    "            names = []\n",
    "            for j in range(TOP_K_SAVE):\n",
    "                rec[f\"prob_{j}\"] = 0.0\n",
    "\n",
    "            if out.outputs:\n",
    "                cand_dict = out.outputs[0].logprobs[0] if out.outputs[0].logprobs else None\n",
    "                cand_list = []\n",
    "                if isinstance(cand_dict, dict):\n",
    "                    # token_id は int なので、上位kをそのまま拾う\n",
    "                    # decode->[数字文字列]->int->all_completions[idx]\n",
    "                    for token_id, obj in cand_dict.items():\n",
    "                        logp = getattr(obj, \"logprob\", None)\n",
    "                        if logp is None:\n",
    "                            try:\n",
    "                                logp = float(obj)\n",
    "                            except Exception:\n",
    "                                continue\n",
    "                        try:\n",
    "                            s = tokenizer_hf.decode([int(token_id)]).strip()\n",
    "                        except Exception:\n",
    "                            s = str(token_id)\n",
    "                        if s.isdigit():\n",
    "                            idx = int(s)\n",
    "                            if 0 <= idx < len(all_completions):\n",
    "                                cand_list.append((all_completions[idx], float(np.exp(logp))))\n",
    "                cand_list.sort(key=lambda x: -x[1])\n",
    "                cand_list = cand_list[:min(TOP_K_SAVE, 20)]\n",
    "                for j in range(len(cand_list)):\n",
    "                    rec[f\"prob_{j}\"] = cand_list[j][1]\n",
    "                    names.append(cand_list[j][0])\n",
    "\n",
    "            rec[\"top_classes\"] = \" \".join(names)\n",
    "            prob_rows.append(rec)\n",
    "            topk_names.append(names[:TOP_K_SUB])\n",
    "\n",
    "    else:\n",
    "        # ===== Case B: 文字ラベル抽出 =====\n",
    "        label_index = _build_label_index(all_completions)\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.0, top_p=1.0, top_k=-1,\n",
    "            max_tokens=6,\n",
    "            ignore_eos=True,\n",
    "            logprobs=min(20, VLLM_LOGPROBS_K, TOP_K_SAVE),\n",
    "            stop=None, stop_token_ids=None,\n",
    "        )\n",
    "        print(\"[INFO] Generating (Case B)...\")\n",
    "        t0 = time.time()\n",
    "        outputs = list(llm.generate(prompts, sampling_params))\n",
    "        print(f\"[INFO] Generation done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "        for i, out in enumerate(outputs):\n",
    "            rec = {\"row_id\": int(row_ids[i])}\n",
    "            names = []\n",
    "            for j in range(TOP_K_SAVE):\n",
    "                rec[f\"prob_{j}\"] = 0.0\n",
    "\n",
    "            picked_label = None\n",
    "            cand_list = []\n",
    "\n",
    "            if out.outputs:\n",
    "                o0 = out.outputs[0]\n",
    "                tok_ids = getattr(o0, \"token_ids\", None) or []\n",
    "                try:\n",
    "                    raw_text = vtok.decode(tok_ids, skip_special_tokens=False)\n",
    "                except Exception:\n",
    "                    raw_text = tokenizer_hf.decode(tok_ids, skip_special_tokens=False)\n",
    "                text_no_im = IM_SPECIAL.sub(\"\", raw_text).strip()\n",
    "\n",
    "                # ① 文字ラベル優先\n",
    "                picked_label = _pick_label_from_text(text_no_im, label_index)\n",
    "\n",
    "                # ② 数字フォールバック\n",
    "                if picked_label is None:\n",
    "                    m = DIGIT_PREFIX_RE.match(text_no_im or \"\")\n",
    "                    if m:\n",
    "                        idx_num = int(m.group(1))\n",
    "                        if 0 <= idx_num < len(all_completions):\n",
    "                            picked_label = all_completions[idx_num]\n",
    "\n",
    "                # ③ さらにダメなら1ステップ目logprobsから候補\n",
    "                if picked_label is None and o0.logprobs:\n",
    "                    cand_dict = o0.logprobs[0]\n",
    "                    if isinstance(cand_dict, dict):\n",
    "                        for token_id, obj in cand_dict.items():\n",
    "                            logp = getattr(obj, \"logprob\", None)\n",
    "                            if logp is None:\n",
    "                                try:\n",
    "                                    logp = float(obj)\n",
    "                                except Exception:\n",
    "                                    continue\n",
    "                            tok_str = getattr(obj, \"decoded_token\", None)\n",
    "                            if tok_str is None:\n",
    "                                try:\n",
    "                                    tid = int(token_id)\n",
    "                                    tok_str = tokenizer_hf.decode([tid])\n",
    "                                except Exception:\n",
    "                                    tok_str = str(token_id)\n",
    "                            tok_str = (tok_str or \"\").strip()\n",
    "                            lab = _pick_label_from_text(tok_str, label_index)\n",
    "                            if lab is not None:\n",
    "                                cand_list.append((lab, float(np.exp(logp))))\n",
    "                            elif tok_str.isdigit():\n",
    "                                idx_num = int(tok_str)\n",
    "                                if 0 <= idx_num < len(all_completions):\n",
    "                                    cand_list.append((all_completions[idx_num], float(np.exp(logp))))\n",
    "                        cand_list.sort(key=lambda x: -x[1])\n",
    "                        cand_list = cand_list[:min(TOP_K_SAVE, 20)]\n",
    "\n",
    "            if picked_label is not None:\n",
    "                names.append(picked_label)\n",
    "                rec[\"prob_0\"] = 1.0\n",
    "            else:\n",
    "                for j in range(len(cand_list)):\n",
    "                    rec[f\"prob_{j}\"] = cand_list[j][1]\n",
    "                    names.append(cand_list[j][0])\n",
    "\n",
    "            rec[\"top_classes\"] = \" \".join(names)\n",
    "            prob_rows.append(rec)\n",
    "            topk_names.append(names[:TOP_K_SUB])\n",
    "\n",
    "    # --- 保存 ---\n",
    "    prob_df = pd.DataFrame(prob_rows)\n",
    "    sub_df = pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"Category:Misconception\": [\" \".join(x) for x in topk_names]\n",
    "    })\n",
    "    prob_path = os.path.join(OUT_DIR, \"submission_qwen_sft_probabilities.csv\")\n",
    "    sub_path  = os.path.join(OUT_DIR, \"submission_qwen_sft.csv\")\n",
    "    prob_df.to_csv(prob_path, index=False)\n",
    "    sub_df.to_csv(sub_path, index=False)\n",
    "    print(f\"[INFO] Saved probabilities to {prob_path}\")\n",
    "    print(f\"[INFO] Saved submission to   {sub_path}\")\n",
    "    print(\"[INFO] Preview probabilities:\")\n",
    "    print(prob_df.head())\n",
    "\n",
    "    # --- 後片付け ---\n",
    "    try:\n",
    "        del llm, tokenizer_hf, prob_rows, topk_names, prompts, row_ids, all_completions\n",
    "    except Exception:\n",
    "        pass\n",
    "    _safe_dist_destroy()\n",
    "    _sync_and_free()\n",
    "    print(\"[INFO] Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d03d25a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T01:48:38.438838Z",
     "iopub.status.busy": "2025-10-09T01:48:38.438451Z",
     "iopub.status.idle": "2025-10-09T01:48:38.456363Z",
     "shell.execute_reply": "2025-10-09T01:48:38.455552Z"
    },
    "papermill": {
     "duration": 0.026036,
     "end_time": "2025-10-09T01:48:38.457622",
     "exception": false,
     "start_time": "2025-10-09T01:48:38.431586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing resnet_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile resnet_inference.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Text Processing and Feature Extraction\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "# Point NLTK to the offline dataset\n",
    "nltk.data.path.append('/kaggle/input/nltk-data') \n",
    "# Preprocessing, Modeling, and Evaluation\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib\n",
    "\n",
    "# Models\n",
    "\n",
    "# --- PyTorch imports for ResNet multitask model ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Fix: Import torch.nn.functional as F for Attention class\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# --- Configuration Class ---\n",
    "class CFG:\n",
    "    train = False\n",
    "\n",
    "    DEBUG = False\n",
    "    nn_architecture = 'AttentionMLP' \n",
    "    vectorizer_name = 'SentenceBERT'\n",
    "    random_state = 42\n",
    "    n_splits = 10 # We can now use 10 folds safely\n",
    "\n",
    "    sbert_model_path = \"/kaggle/input/all-minilm-l6-v2/all-MiniLM-L6-v2\"\n",
    "\n",
    "# --- Set all random seeds for reproducibility ---\n",
    "def set_seed(seed=42):\n",
    "    import random\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(CFG.random_state)\n",
    "\n",
    "def map_at_3(y_true, y_pred_top3):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Average Precision @ 3 (MAP@3).\n",
    "    y_true: array-like of shape (n_samples,)\n",
    "    y_pred_top3: array-like of shape (n_samples, 3)\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    for true_label, pred_labels in zip(y_true, y_pred_top3):\n",
    "        pred_labels = list(pred_labels)\n",
    "        ap = 0.0\n",
    "        if true_label in pred_labels[:3]:\n",
    "            rank = pred_labels.index(true_label) + 1\n",
    "            ap = 1 / rank\n",
    "        average_precisions.append(ap)\n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_only(original: str) -> str:\n",
    "    \"\"\"Return ONLY the normalized companion text (do not rewrite original).\"\"\"\n",
    "    norm = str(original)\n",
    "\n",
    "    # strip LaTeX wrappers\n",
    "    norm = re.sub(r'\\\\\\(|\\\\\\)|\\$\\$?', '', norm)\n",
    "\n",
    "    # \\frac{a}{b} -> \"a over b\"\n",
    "    norm = re.sub(r'\\\\frac\\s*{([^}]*)}\\s*{([^}]*)}', lambda m: f\"{m.group(1).strip()} over {m.group(2).strip()}\", norm)\n",
    "\n",
    "    # operators (keep hyphens in words; only replace numeric minus)\n",
    "    norm = norm.replace('\\\\div', ' divided by ')\n",
    "    norm = norm.replace('×', ' times ').replace('÷', ' divided by ')\n",
    "    norm = re.sub(r'\\s*=\\s*', ' equals ', norm)\n",
    "    norm = re.sub(r'(?<=\\d)\\s*\\+\\s*(?=\\d)', ' plus ', norm)\n",
    "    norm = re.sub(r'(?<=\\d)\\s*\\*\\s*(?=\\d)', ' times ', norm)\n",
    "    norm = re.sub(r'(?<=\\d)\\s*-\\s*(?=\\d)', ' minus ', norm)   # 3-2 -> \"3 minus 2\"\n",
    "    norm = re.sub(r'(?<!\\w)-(?=\\d)', ' minus ', norm)         # -5 -> \"minus 5\"\n",
    "    norm = norm.replace('^', ' to the power of ')\n",
    "\n",
    "    # image linearization\n",
    "    norm = re.sub(r'\\[Image:(.*?)\\]', r'[image: \\1]', norm, flags=re.I)\n",
    "\n",
    "    # collapse spaces\n",
    "    norm = re.sub(r'\\s+', ' ', norm).strip()\n",
    "    return norm\n",
    "\n",
    "def combine_for_encoding(original: str, norm: str) -> str:\n",
    "    \"\"\"What you feed to SentenceTransformer (original preserved + normalized).\"\"\"\n",
    "    return f\"{original.strip()} || Normalized: {norm.strip()}\"\n",
    "\n",
    "def categorize_math_topic(question_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Categorizes a question into a math topic based on keywords.\n",
    "    \"\"\"\n",
    "    # text = normalize_text_for_topics(question_text)\n",
    "    text = question_text\n",
    "    # Check for the most specific topics first\n",
    "    if 'probability' in text or 'likelihood' in text:\n",
    "        return 'Probability'\n",
    "    if 'polygon' in text or 'angle' in text or 'sides' in text:\n",
    "        return 'Geometry'\n",
    "    if 'pattern' in text:\n",
    "        return 'Patterns'\n",
    "    if 'greatest' in text or 'larger' in text:\n",
    "        return 'Comparison'\n",
    "    if ' y=' in text or 'value of' in text or ' a=' in text:\n",
    "        return 'Algebra'\n",
    "    if 'fraction' in text or 'shaded' in text or '/' in text or '÷' in text or ' of the ' in text:\n",
    "        return 'Fractions_And_Ratios'\n",
    "    if 'calculate' in text or '+' in text or '-' in text or '*' in text:\n",
    "        return 'Arithmetic'\n",
    "    \n",
    "    return 'Other' # A catch-all for anything else\n",
    "\n",
    "def get_question_archetype(question_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Identifies the archetype of a math question based on its structure and keywords.\n",
    "    \"\"\"\n",
    "    if not isinstance(question_text, str):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Make text lowercase for consistent matching\n",
    "    text_lower = question_text.lower()\n",
    "    \n",
    "    # 1. Direct Calculation\n",
    "    # Matches questions that start with \"calculate\" or are just an expression\n",
    "    if text_lower.startswith('calculate') or (text_lower.endswith('=') and len(text_lower.split()) < 7):\n",
    "        return 'Calculation'\n",
    "    \n",
    "    # 2. Solve for Variable\n",
    "    if 'what is the value of' in text_lower:\n",
    "        return 'Solve_for_Variable'\n",
    "        \n",
    "    # 3. Comparison\n",
    "    if 'which number is the greatest' in text_lower:\n",
    "        return 'Comparison'\n",
    "    \n",
    "    # 4. Pattern Recognition\n",
    "    if 'pattern' in text_lower:\n",
    "        return 'Pattern'\n",
    "    \n",
    "    # 5. Word Problem (Heuristic: longer, multi-sentence questions)\n",
    "    # This can be a default for longer questions that don't fit other types\n",
    "    if len(question_text.split()) > 15:\n",
    "        return 'Word_Problem'\n",
    "        \n",
    "    # Default catch-all\n",
    "    return 'Other'\n",
    "\n",
    "def extract_numerical_features(df):\n",
    "    \"\"\"\n",
    "    Extracts numerical features from QuestionText and MC_Answer columns.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Define a regex to find integers and decimals\n",
    "    num_regex = r'\\d+\\.?\\d*'\n",
    "    df_out['has_image'] = df_out['QuestionText'].str.contains(r'\\[image:', case=False, na=False).astype(int)\n",
    "\n",
    "    # Find all numbers in the text columns\n",
    "    nums_in_question = df_out['QuestionText'].astype(str).str.findall(num_regex)\n",
    "    nums_in_answer = df_out['MC_Answer'].astype(str).str.findall(num_regex)\n",
    "\n",
    "    # 1. Feature: Count of numbers in the question\n",
    "    df_out['question_num_count'] = nums_in_question.str.len()\n",
    "\n",
    "    # 2. Feature: Does the answer contain a number?\n",
    "    df_out['answer_has_num'] = (nums_in_answer.str.len() > 0).astype(int)\n",
    "\n",
    "    # 3. Feature: Is a number from the answer also in the question?\n",
    "    def check_intersection(row):\n",
    "        # Using sets for efficient intersection checking\n",
    "        set_q_nums = set(row[0])\n",
    "        set_a_nums = set(row[1])\n",
    "        return 1 if not set_q_nums.isdisjoint(set_a_nums) else 0\n",
    "\n",
    "    df_out['answer_num_in_question'] = pd.Series(zip(nums_in_question, nums_in_answer)).apply(check_intersection)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def create_features_corrected(df):\n",
    "    df = df.copy()\n",
    "    for col in ['QuestionText', 'MC_Answer', 'StudentExplanation']:\n",
    "        # print(f\"Unique values in {col}: {df[col].unique()}\")\n",
    "        df[f'{col}_N'] = df[col].astype(str).apply(normalize_only)\n",
    "        # print(f\"Unique values in {col}_N: {df[f'{col}_N'].unique()}\")\n",
    "        df[f'{col}_to_encode'] = df.apply(\n",
    "            lambda r: combine_for_encoding(r[col], r[f'{col}_N']), axis=1\n",
    "        )\n",
    "        # df[f'{col}_to_encode'] = df[col]\n",
    "\n",
    "\n",
    "    df['math_topic'] = df['QuestionText'].astype(str).apply(categorize_math_topic)\n",
    "    df['question_type'] = df['QuestionText'].astype(str).apply(get_question_archetype)\n",
    "\n",
    "    def format_input(row):\n",
    "        x = \"Yes\"\n",
    "        if not row.get('is_correct', 0):\n",
    "            x = \"No\"\n",
    "        return (\n",
    "            f\"Question: {row['QuestionText']}\\n\"\n",
    "            f\"Answer: {row['MC_Answer']}\\n\"\n",
    "            f\"Correct? {x}\\n\"\n",
    "            # f\"Student Explanation: {row['StudentExplanation']}\"\n",
    "        )\n",
    "    \n",
    "    # df['problem_context'] = df.apply(format_input, axis=1)\n",
    "\n",
    "    # # Embedding 1: Combine Question and the selected Multiple Choice answer\n",
    "    df['problem_context'] = \"Question: \" + df['QuestionText_to_encode'] + \" | Answer: \" + df['MC_Answer_to_encode']\n",
    "    \n",
    "    # Embedding 2: The student's reasoning\n",
    "    df['explanation_text'] = \"Explanation: \" + df['StudentExplanation']\n",
    "    df = extract_numerical_features(df)\n",
    "    # Clean up intermediate columns before returning\n",
    "    df.drop(columns=['QuestionText_N', 'MC_Answer_N', 'StudentExplanation_N',\n",
    "                     'QuestionText_to_encode', 'MC_Answer_to_encode', 'StudentExplanation_to_encode'], \n",
    "            inplace=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "sbert_model = None\n",
    "if CFG.vectorizer_name == 'SentenceBERT':\n",
    "    print(f\"Loading Sentence-BERT model from: {CFG.sbert_model_path}\")\n",
    "    sbert_model = SentenceTransformer(CFG.sbert_model_path)\n",
    "\n",
    "def get_sbert_features(texts, model):\n",
    "    \"\"\"\n",
    "    Generates sentence embeddings using a pre-loaded Sentence-BERT model.\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(texts, show_progress_bar=False)\n",
    "    return np.array(embeddings)\n",
    "    \n",
    "# --- Main Script ---\n",
    "\n",
    "# 1. Load Data\n",
    "input_dir = Path('/kaggle/input/map-charting-student-math-misunderstandings')\n",
    "train_df = pd.read_csv(input_dir / 'train.csv')\n",
    "test_df = pd.read_csv(input_dir / 'test.csv')\n",
    "if CFG.DEBUG:\n",
    "    train_df = train_df.sample(n=1000, random_state=CFG.random_state).reset_index(drop=True)\n",
    "\n",
    "idx = train_df.apply(lambda row: row.Category.split('_')[0], axis=1) == 'True'\n",
    "correct = train_df.loc[idx].copy()\n",
    "correct['c'] = correct.groupby(['QuestionId', 'MC_Answer']).MC_Answer.transform('count')\n",
    "correct = correct.sort_values('c', ascending=False)\n",
    "correct = correct.drop_duplicates(['QuestionId'])\n",
    "correct = correct[['QuestionId', 'MC_Answer']]\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "test_df = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "test_df = test_df.merge(correct, on=['QuestionId', 'MC_Answer'], how='left')\n",
    "test_df['is_correct'] = test_df['is_correct'].fillna(0)\n",
    "\n",
    "train_df = train_df.merge(correct, on=['QuestionId', 'MC_Answer'], how='left')\n",
    "train_df['is_correct'] = train_df['is_correct'].fillna(0)\n",
    "\n",
    "# Display columns before feature engineering\n",
    "print(\"Train columns BEFORE feature engineering:\", list(train_df.columns))\n",
    "print(\"Test columns BEFORE feature engineering:\", list(test_df.columns))\n",
    "\n",
    "# 2. Feature Engineering and Label Creation\n",
    "print(\"Creating and cleaning text features...\")\n",
    "train_df = create_features_corrected(train_df)\n",
    "test_df = create_features_corrected(test_df)\n",
    "\n",
    "# Display columns after feature engineering\n",
    "print(\"Train columns AFTER feature engineering:\", list(train_df.columns))\n",
    "print(\"Test columns AFTER feature engineering:\", list(test_df.columns))\n",
    "\n",
    "# Display number of features added\n",
    "num_added_train = len(train_df.columns) - len(pd.read_csv(input_dir / 'train.csv').columns)\n",
    "num_added_test = len(test_df.columns) - len(pd.read_csv(input_dir / 'test.csv').columns)\n",
    "print(f\"Number of features added to train_df: {num_added_train}\")\n",
    "print(f\"Number of features added to test_df: {num_added_test}\")\n",
    "\n",
    "# Display value counts for key engineered features\n",
    "feature_cols = ['QuestionId', 'math_topic', 'question_type', 'has_image', 'question_num_count', 'answer_has_num', 'answer_num_in_question']\n",
    "for col in feature_cols:\n",
    "    print(f\"\\nValue counts for '{col}':\")\n",
    "    print(train_df[col].value_counts())\n",
    "\n",
    "train_df['Misconception'] = train_df['Misconception'].fillna('NA')\n",
    "train_df['label'] = train_df['Category'] + ':' + train_df['Misconception']\n",
    "\n",
    "def get_row_slice(X, idx):\n",
    "    return X[idx, :]\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, p=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.fc1(x))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "class ResNetMLP_MultiTask(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_blocks, dropout_rate, num_label_classes, num_cat_classes, num_misc_classes):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.blocks = nn.Sequential(*[ResidualBlock(hidden_dim, p=dropout_rate) for _ in range(num_blocks)])\n",
    "        self.label_head = nn.Linear(hidden_dim, num_label_classes)\n",
    "        self.cat_head = nn.Linear(hidden_dim, num_cat_classes)\n",
    "        self.misc_head = nn.Linear(hidden_dim, num_misc_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.blocks(x)\n",
    "        label_logits = self.label_head(x)\n",
    "        cat_logits = self.cat_head(x)\n",
    "        misc_logits = self.misc_head(x)\n",
    "        return label_logits, cat_logits, misc_logits\n",
    "    \n",
    "# --- Multi-task train_resnet_model for classification ---\n",
    "def train_resnet_multitask_model(\n",
    "    X_train, y_label_train, y_cat_train, y_misc_train,\n",
    "    X_val, y_label_val, y_cat_val, y_misc_val,\n",
    "    model, training_params\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the ResNetMLP_MultiTask model for multi-task classification.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "        torch.tensor(y_label_train, dtype=torch.long),\n",
    "        torch.tensor(y_cat_train, dtype=torch.long),\n",
    "        torch.tensor(y_misc_train, dtype=torch.long)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "        torch.tensor(y_label_val, dtype=torch.long),\n",
    "        torch.tensor(y_cat_val, dtype=torch.long),\n",
    "        torch.tensor(y_misc_val, dtype=torch.long)\n",
    "    )\n",
    "\n",
    "    import os\n",
    "    num_workers = os.cpu_count() if os.cpu_count() is not None else 0\n",
    "    train_loader = DataLoader(train_dataset, batch_size=training_params['batch_size'], shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=training_params['batch_size'], shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=training_params['learning_rate'],\n",
    "        weight_decay=training_params.get('weight_decay', 0)\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = training_params.get('patience', 20)\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(training_params['epochs']):\n",
    "        model.train()\n",
    "        for inputs, y_label, y_cat, y_misc in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            y_label = y_label.to(device)\n",
    "            y_cat = y_cat.to(device)\n",
    "            y_misc = y_misc.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            label_logits, cat_logits, misc_logits = model(inputs)\n",
    "            loss_label = loss_fn(label_logits, y_label)\n",
    "            loss_cat = loss_fn(cat_logits, y_cat)\n",
    "            loss_misc = loss_fn(misc_logits, y_misc)\n",
    "            loss = loss_label + loss_cat + loss_misc\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, y_label, y_cat, y_misc in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                y_label = y_label.to(device)\n",
    "                y_cat = y_cat.to(device)\n",
    "                y_misc = y_misc.to(device)\n",
    "                label_logits, cat_logits, misc_logits = model(inputs)\n",
    "                loss_label = loss_fn(label_logits, y_label)\n",
    "                loss_cat = loss_fn(cat_logits, y_cat)\n",
    "                loss_misc = loss_fn(misc_logits, y_misc)\n",
    "                loss = loss_label + loss_cat + loss_misc\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), training_params['save_path'])\n",
    "            epochs_no_improve = 0\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}: Validation loss decreased. Model saved to {training_params['save_path']}\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}: Validation loss did not improve.\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(training_params['save_path'], map_location=device))\n",
    "    print(\"Training finished.\")\n",
    "    return model\n",
    "\n",
    "def train_and_predict_resnet_multitask(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Train and predict with ResNetMLP_MultiTask using cross-validation.\n",
    "    Returns ensemble test predictions from the joint label head and prints per-fold/mean accuracy and MAP@3.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    label_encoder = LabelEncoder()\n",
    "    cat_encoder = LabelEncoder()\n",
    "    misc_encoder = LabelEncoder()\n",
    "    train_labels_encoded = label_encoder.fit_transform(train_df['label'])\n",
    "    train_cat_encoded = cat_encoder.fit_transform(train_df['Category'])\n",
    "    train_misc_encoded = misc_encoder.fit_transform(train_df['Misconception'])\n",
    "\n",
    "    joblib.dump(label_encoder, 'label_encoder.joblib')\n",
    "    joblib.dump(cat_encoder, 'cat_encoder.joblib')\n",
    "    joblib.dump(misc_encoder, 'misc_encoder.joblib')\n",
    "\n",
    "    # --- Save vectorizer after fitting ---\n",
    "    # --- Precompute all features outside the CV loop ---\n",
    "\n",
    "    # Create embeddings for the problem context\n",
    "    X_context_all = get_sbert_features(train_df['problem_context'], sbert_model)\n",
    "    X_context_test = get_sbert_features(test_df['problem_context'], sbert_model)\n",
    "    # Create embeddings for the explanation\n",
    "    X_explanation_all = get_sbert_features(train_df['explanation_text'], sbert_model)\n",
    "    X_explanation_test = get_sbert_features(test_df['explanation_text'], sbert_model)\n",
    "\n",
    "\n",
    "    cat_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    # Include both 'QuestionId' and 'math_topic' in the categorical encoding\n",
    "    columns_to_encode = ['QuestionId', 'math_topic', 'question_type', 'has_image', 'question_num_count', 'answer_has_num', 'answer_num_in_question', 'is_correct']\n",
    "    print(\"columns_to_encode:\", columns_to_encode)\n",
    "    X_cat_all = cat_encoder.fit_transform(train_df[columns_to_encode])\n",
    "    X_cat_test = cat_encoder.transform(test_df[columns_to_encode])\n",
    "\n",
    "    # In the CV loop, save the model for each fold\n",
    "    kf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.random_state)\n",
    "    val_accuracies = []\n",
    "    val_map3s = []\n",
    "    test_predictions = np.zeros((len(test_df), len(label_encoder.classes_)))\n",
    "\n",
    "    print(f\"Starting {CFG.n_splits}-fold cross-validation with {CFG.nn_architecture}...\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_folds = kf.get_n_splits()\n",
    "    X_test_all = np.hstack([X_context_test, X_explanation_test, X_cat_test])\n",
    "    val_accuracies = []\n",
    "    val_map3s = []\n",
    "    test_predictions = np.zeros((len(X_test_all), len(label_encoder.classes_)))\n",
    "\n",
    "    input_dim = X_context_all.shape[1] + X_explanation_all.shape[1] + X_cat_all.shape[1]\n",
    "    model_params = dict(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=256,\n",
    "        num_blocks=20,\n",
    "        dropout_rate=0.2,\n",
    "        num_label_classes=len(label_encoder.classes_),\n",
    "        num_cat_classes=len(cat_encoder.classes_) if hasattr(cat_encoder, 'classes_') else len(set(train_df['Category'])),\n",
    "        num_misc_classes=len(misc_encoder.classes_) if hasattr(misc_encoder, 'classes_') else len(set(train_df['Misconception']))\n",
    "    )\n",
    "    training_params_base = dict(\n",
    "        batch_size=128,\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=1e-4,\n",
    "        epochs=500,\n",
    "        patience=10\n",
    "    )\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, train_labels_encoded)):\n",
    "        print(f\"--- Fold {fold+1}/{n_folds} (ResNet MultiTask) ---\")\n",
    "        y_label_tr = train_labels_encoded[train_idx]\n",
    "        y_label_val = train_labels_encoded[val_idx]\n",
    "        y_cat_tr = train_cat_encoded[train_idx]\n",
    "        y_cat_val = train_cat_encoded[val_idx]\n",
    "        y_misc_tr = train_misc_encoded[train_idx]\n",
    "        y_misc_val = train_misc_encoded[val_idx]\n",
    "        X_tr_context = X_context_all[train_idx]\n",
    "        X_val_context = X_context_all[val_idx]\n",
    "        X_tr_explanation = X_explanation_all[train_idx]\n",
    "        X_val_explanation = X_explanation_all[val_idx]\n",
    "        X_tr_cat = X_cat_all[train_idx]\n",
    "        X_val_cat = X_cat_all[val_idx]\n",
    "        X_tr_all = np.hstack([X_tr_context, X_tr_explanation, X_tr_cat])\n",
    "        X_val_all = np.hstack([X_val_context, X_val_explanation, X_val_cat])\n",
    "        training_params = training_params_base.copy()\n",
    "        training_params['save_path'] = f'resnet_multitask_fold{fold+1}.pt'\n",
    "        model = ResNetMLP_MultiTask(**model_params).to(device)\n",
    "\n",
    "        train_resnet_multitask_model(\n",
    "            X_tr_all, y_label_tr, y_cat_tr, y_misc_tr,\n",
    "            X_val_all, y_label_val, y_cat_val, y_misc_val,\n",
    "            model, training_params\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(torch.load(training_params['save_path'], map_location=device))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits, _, _ = model(torch.tensor(X_val_all, dtype=torch.float32).to(device))\n",
    "            val_probs = torch.softmax(val_logits, dim=1).cpu().numpy()\n",
    "            val_preds = np.argmax(val_probs, axis=1)\n",
    "            val_acc = accuracy_score(y_label_val, val_preds)\n",
    "            val_top3 = np.argsort(val_probs, axis=1)[:, -3:][:, ::-1]\n",
    "            val_map3 = map_at_3(y_label_val, val_top3)\n",
    "            val_accuracies.append(val_acc)\n",
    "            val_map3s.append(val_map3)\n",
    "            print(f\"Fold {fold+1}: val acc = {val_acc:.4f}, MAP@3 = {val_map3:.4f}\")\n",
    "            test_logits, _, _ = model(torch.tensor(X_test_all, dtype=torch.float32).to(device))\n",
    "            test_proba_fold = torch.softmax(test_logits, dim=1).cpu().numpy()\n",
    "            test_predictions += test_proba_fold / n_folds\n",
    "\n",
    "    print(f\"\\nMean val acc: {np.mean(val_accuracies):.4f}  Mean MAP@3: {np.mean(val_map3s):.4f}\\n\")\n",
    "    return test_predictions, label_encoder\n",
    "\n",
    "# Store predictions for each target\n",
    "\n",
    "# Define model and training parameters\n",
    "\n",
    "\n",
    "# Add these necessary imports at the top of your script\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_predict(test_df):\n",
    "    \"\"\"\n",
    "    Loads trained models and makes predictions. It recreates the feature encoder \n",
    "    by fitting it on the original training data, as this encoder was not saved.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    from pathlib import Path\n",
    "    \n",
    "    model_dir  = '/kaggle/input/map-math/map-resnetmlp-multitask-v4'\n",
    "\n",
    "    # --- Load Encoders ---\n",
    "    label_encoder = joblib.load(os.path.join(model_dir, 'label_encoder.joblib'))\n",
    "    misc_encoder = joblib.load(os.path.join(model_dir, 'misc_encoder.joblib'))\n",
    "    # Note: We will not use 'cat_encoder.joblib' as it's the wrong type for features.\n",
    "\n",
    "    # --- SBERT Embeddings ---\n",
    "    sbert_model = SentenceTransformer(CFG.sbert_model_path)\n",
    "    X_context_test = get_sbert_features(test_df['problem_context'], sbert_model)\n",
    "    X_explanation_test = get_sbert_features(test_df['explanation_text'], sbert_model)\n",
    "    \n",
    "    # --- Recreate the OneHotEncoder by fitting it on the training data ---\n",
    "    # 1. Load the original training data to ensure consistent encoding\n",
    "    input_dir = Path('/kaggle/input/map-charting-student-math-misunderstandings')\n",
    "    train_df_original = pd.read_csv(input_dir / 'train.csv')\n",
    "\n",
    "    # 2. Replicate the 'is_correct' feature creation from the main script\n",
    "    idx = train_df_original.apply(lambda row: row.Category.split('_')[0], axis=1) == 'True'\n",
    "    correct = train_df_original.loc[idx].copy()\n",
    "    correct['c'] = correct.groupby(['QuestionId', 'MC_Answer']).MC_Answer.transform('count')\n",
    "    correct = correct.sort_values('c', ascending=False)\n",
    "    correct = correct.drop_duplicates(['QuestionId'])\n",
    "    correct = correct[['QuestionId', 'MC_Answer']]\n",
    "    correct['is_correct'] = 1\n",
    "    train_df_original = train_df_original.merge(correct, on=['QuestionId', 'MC_Answer'], how='left')\n",
    "    train_df_original['is_correct'] = train_df_original['is_correct'].fillna(0)\n",
    "\n",
    "    # 3. Apply the same full feature engineering to the loaded training data\n",
    "    train_df_features = create_features_corrected(train_df_original)\n",
    "\n",
    "    # 4. Create a new OneHotEncoder, fit it on the training features, and transform the test features\n",
    "    columns_to_encode = ['QuestionId', 'math_topic', 'question_type', 'has_image', 'question_num_count', 'answer_has_num', 'answer_num_in_question', 'is_correct']\n",
    "    feature_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    feature_encoder.fit(train_df_features[columns_to_encode])\n",
    "    X_cat_test = feature_encoder.transform(test_df[columns_to_encode])\n",
    "\n",
    "    # --- Combine all features for prediction ---\n",
    "    X_test_all = np.hstack([X_context_test, X_explanation_test, X_cat_test])\n",
    "\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    test_predictions = np.zeros((len(test_df), num_classes))\n",
    "\n",
    "    # --- Model Parameters and Inference Loop ---\n",
    "    input_dim = X_test_all.shape[1]\n",
    "    model_params = dict(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=256,\n",
    "        num_blocks=20,\n",
    "        dropout_rate=0.2,\n",
    "        num_label_classes=len(label_encoder.classes_),\n",
    "        # The number of classes for cat/misc heads doesn't affect inference for the primary 'label' head\n",
    "        num_cat_classes=len(train_df_features['Category'].unique()),\n",
    "        num_misc_classes=len(train_df_features['Misconception'].unique())\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Define n_folds based on the config used during training\n",
    "    n_folds = CFG.n_splits\n",
    "    \n",
    "    # Ensemble predictions from n_folds\n",
    "    for fold in range(1, n_folds + 1):\n",
    "        model = ResNetMLP_MultiTask(**model_params).to(device)\n",
    "        model_path = Path(model_dir) / f'resnet_multitask_fold{fold}.pt'\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_inputs = torch.tensor(X_test_all, dtype=torch.float32).to(device)\n",
    "            label_logits, _, _ = model(test_inputs)\n",
    "            proba = torch.softmax(label_logits, dim=1).cpu().numpy()\n",
    "            test_predictions += proba / n_folds\n",
    "            \n",
    "    return test_predictions, label_encoder\n",
    "\n",
    "if CFG.train:\n",
    "    test_predictions, label_encoder = train_and_predict_resnet_multitask(train_df, test_df)\n",
    "else:\n",
    "    test_predictions, label_encoder = load_and_predict(test_df)\n",
    "\n",
    "top3_indices = np.argsort(test_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "top3_labels = label_encoder.inverse_transform(top3_indices.flatten()).reshape(top3_indices.shape)\n",
    "# Get top 25 predictions (indices, labels, and probabilities) for the new model\n",
    "print(\"Saving ranked probabilities for the ResNet model...\")\n",
    "top_indices_full = np.argsort(test_predictions, axis=1)[:, ::-1][:, :25]\n",
    "top_labels_full = label_encoder.inverse_transform(top_indices_full.flatten()).reshape(top_indices_full.shape)\n",
    "\n",
    "# Get the probability values for the top predictions\n",
    "top_probs_full = np.array([test_predictions[i, top_indices_full[i]] for i in range(len(test_predictions))])\n",
    "\n",
    "prob_data = []\n",
    "for i in range(len(test_df)):\n",
    "    prob_dict = {f\"prob_{j}\": top_probs_full[i, j] for j in range(25)}\n",
    "    prob_dict['row_id'] = test_df.row_id.values[i]\n",
    "    prob_dict['top_classes'] = \" \".join(top_labels_full[i, :25])\n",
    "    prob_data.append(prob_dict)\n",
    "\n",
    "prob_df = pd.DataFrame(prob_data)\n",
    "prob_df.to_csv(\"submission_resnet_probabilities.csv\", index=False)\n",
    "print(\"ResNet probability file created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24718f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T01:48:38.469888Z",
     "iopub.status.busy": "2025-10-09T01:48:38.469581Z",
     "iopub.status.idle": "2025-10-09T01:48:38.480363Z",
     "shell.execute_reply": "2025-10-09T01:48:38.479420Z"
    },
    "papermill": {
     "duration": 0.018815,
     "end_time": "2025-10-09T01:48:38.481782",
     "exception": false,
     "start_time": "2025-10-09T01:48:38.462967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing xgboost_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile xgboost_inference.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Text Processing and Feature Extraction\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Point NLTK to the offline dataset\n",
    "nltk.data.path.append('/kaggle/input/nltk-data') \n",
    "\n",
    "# Preprocessing, Modeling, and Evaluation\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration Class ---\n",
    "class CFG:\n",
    "    model_name = 'XGBoost' # Options: 'LightGBM', 'XGBoost', 'CatBoost'\n",
    "    vectorizer_name = 'SentenceBERT'\n",
    "    random_state = 42\n",
    "    n_splits = 10 # We can now use 10 folds safely\n",
    "\n",
    "    sbert_model_path = \"/kaggle/input/all-minilm-l6-v2/all-MiniLM-L6-v2\"\n",
    "\n",
    "# --- Advanced Preprocessing ---\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def map_at_3(y_true, y_pred_top3):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Average Precision @ 3 (MAP@3).\n",
    "    y_true: array-like of shape (n_samples,)\n",
    "    y_pred_top3: array-like of shape (n_samples, 3)\n",
    "    \"\"\"\n",
    "    average_precisions = []\n",
    "    for true_label, pred_labels in zip(y_true, y_pred_top3):\n",
    "        pred_labels = list(pred_labels)\n",
    "        ap = 0.0\n",
    "        if true_label in pred_labels[:3]:\n",
    "            rank = pred_labels.index(true_label) + 1\n",
    "            ap = 1 / rank\n",
    "        average_precisions.append(ap)\n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "def preprocess_text_advanced(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s\\.\\-\\+\\*\\=\\>\\<\\(\\)]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_only(original: str) -> str:\n",
    "    \"\"\"Return ONLY the normalized companion text (do not rewrite original).\"\"\"\n",
    "    norm = str(original)\n",
    "\n",
    "    # strip LaTeX wrappers\n",
    "    norm = re.sub(r'\\\\\\(|\\\\\\)|\\$\\$?', '', norm)\n",
    "\n",
    "    # \\frac{a}{b} -> \"a over b\"\n",
    "    norm = re.sub(r'\\\\frac\\s*{([^}]*)}\\s*{([^}]*)}', lambda m: f\"{m.group(1).strip()} over {m.group(2).strip()}\", norm)\n",
    "\n",
    "    # operators (keep hyphens in words; only replace numeric minus)\n",
    "    norm = norm.replace('\\\\div', ' divided by ')\n",
    "    norm = norm.replace('×', ' times ').replace('÷', ' divided by ')\n",
    "    norm = re.sub(r'\\s*=\\s*', ' equals ', norm)\n",
    "    norm = re.sub(r'(?<=\\d)\\s*\\+\\s*(?=\\d)', ' plus ', norm)\n",
    "    norm = re.sub(r'(?<=\\d)\\s*\\*\\s*(?=\\d)', ' times ', norm)\n",
    "    norm = re.sub(r'(?<=\\d)\\s*-\\s*(?=\\d)', ' minus ', norm)   # 3-2 -> \"3 minus 2\"\n",
    "    norm = re.sub(r'(?<!\\w)-(?=\\d)', ' minus ', norm)         # -5 -> \"minus 5\"\n",
    "    norm = norm.replace('^', ' to the power of ')\n",
    "\n",
    "    # image linearization\n",
    "    norm = re.sub(r'\\[Image:(.*?)\\]', r'[image: \\1]', norm, flags=re.I)\n",
    "\n",
    "    # collapse spaces\n",
    "    norm = re.sub(r'\\s+', ' ', norm).strip()\n",
    "    return norm\n",
    "\n",
    "def combine_for_encoding(original: str, norm: str) -> str:\n",
    "    \"\"\"What you feed to SentenceTransformer (original preserved + normalized).\"\"\"\n",
    "    return f\"{original.strip()} || Normalized: {norm.strip()}\"\n",
    "    # return f\"{norm.strip()}\"\n",
    "\n",
    "def categorize_math_topic(question_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Categorizes a question into a math topic based on keywords.\n",
    "    \"\"\"\n",
    "    # text = normalize_text_for_topics(question_text)\n",
    "    text = question_text\n",
    "    # Check for the most specific topics first\n",
    "    if 'probability' in text or 'likelihood' in text:\n",
    "        return 'Probability'\n",
    "    if 'polygon' in text or 'angle' in text or 'sides' in text:\n",
    "        return 'Geometry'\n",
    "    if 'pattern' in text:\n",
    "        return 'Patterns'\n",
    "    if 'greatest' in text or 'larger' in text:\n",
    "        return 'Comparison'\n",
    "    if ' y=' in text or 'value of' in text or ' a=' in text:\n",
    "        return 'Algebra'\n",
    "    if 'fraction' in text or 'shaded' in text or '/' in text or '÷' in text or ' of the ' in text:\n",
    "        return 'Fractions_And_Ratios'\n",
    "    if 'calculate' in text or '+' in text or '-' in text or '*' in text:\n",
    "        return 'Arithmetic'\n",
    "    \n",
    "    return 'Other' # A catch-all for anything else\n",
    "\n",
    "def get_question_archetype(question_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Identifies the archetype of a math question based on its structure and keywords.\n",
    "    \"\"\"\n",
    "    if not isinstance(question_text, str):\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Make text lowercase for consistent matching\n",
    "    text_lower = question_text.lower()\n",
    "    \n",
    "    # 1. Direct Calculation\n",
    "    # Matches questions that start with \"calculate\" or are just an expression\n",
    "    if text_lower.startswith('calculate') or (text_lower.endswith('=') and len(text_lower.split()) < 7):\n",
    "        return 'Calculation'\n",
    "    \n",
    "    # 2. Solve for Variable\n",
    "    if 'what is the value of' in text_lower:\n",
    "        return 'Solve_for_Variable'\n",
    "        \n",
    "    # 3. Comparison\n",
    "    if 'which number is the greatest' in text_lower:\n",
    "        return 'Comparison'\n",
    "    \n",
    "    # 4. Pattern Recognition\n",
    "    if 'pattern' in text_lower:\n",
    "        return 'Pattern'\n",
    "    \n",
    "    # 5. Word Problem (Heuristic: longer, multi-sentence questions)\n",
    "    # This can be a default for longer questions that don't fit other types\n",
    "    if len(question_text.split()) > 15:\n",
    "        return 'Word_Problem'\n",
    "        \n",
    "    # Default catch-all\n",
    "    return 'Other'\n",
    "\n",
    "def extract_numerical_features(df):\n",
    "    \"\"\"\n",
    "    Extracts numerical features from QuestionText and MC_Answer columns.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Define a regex to find integers and decimals\n",
    "    num_regex = r'\\d+\\.?\\d*'\n",
    "    df_out['has_image'] = df_out['QuestionText'].str.contains(r'\\[image:', case=False, na=False).astype(int)\n",
    "\n",
    "    # Find all numbers in the text columns\n",
    "    nums_in_question = df_out['QuestionText'].astype(str).str.findall(num_regex)\n",
    "    nums_in_answer = df_out['MC_Answer'].astype(str).str.findall(num_regex)\n",
    "\n",
    "    # 1. Feature: Count of numbers in the question\n",
    "    df_out['question_num_count'] = nums_in_question.str.len()\n",
    "\n",
    "    # 2. Feature: Does the answer contain a number?\n",
    "    df_out['answer_has_num'] = (nums_in_answer.str.len() > 0).astype(int)\n",
    "\n",
    "    # 3. Feature: Is a number from the answer also in the question?\n",
    "    def check_intersection(row):\n",
    "        # Using sets for efficient intersection checking\n",
    "        set_q_nums = set(row[0])\n",
    "        set_a_nums = set(row[1])\n",
    "        return 1 if not set_q_nums.isdisjoint(set_a_nums) else 0\n",
    "\n",
    "    df_out['answer_num_in_question'] = pd.Series(zip(nums_in_question, nums_in_answer)).apply(check_intersection)\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def create_features_corrected(df):\n",
    "    df = df.copy()\n",
    "    for col in ['QuestionText', 'MC_Answer', 'StudentExplanation']:\n",
    "        # print(f\"Unique values in {col}: {df[col].unique()}\")\n",
    "        df[f'{col}_N'] = df[col].astype(str).apply(normalize_only)\n",
    "        # print(f\"Unique values in {col}_N: {df[f'{col}_N'].unique()}\")\n",
    "        # --- FIXED: Changed train_df to df ---\n",
    "        df[f'{col}_to_encode'] = df.apply(\n",
    "            lambda r: combine_for_encoding(r[col], r[f'{col}_N']), axis=1\n",
    "        )\n",
    "\n",
    "    df['math_topic'] = df['QuestionText_N'].astype(str).apply(categorize_math_topic)\n",
    "    df['question_type'] = df['QuestionText_N'].astype(str).apply(get_question_archetype)\n",
    "\n",
    "    # Embedding 1: Combine Question and the selected Multiple Choice answer\n",
    "    df['problem_context'] = \"Question: \" + df['QuestionText_to_encode'] + \" | Answer: \" + df['MC_Answer_to_encode']\n",
    "    \n",
    "    # Embedding 2: The student's reasoning\n",
    "    df['explanation_text'] = \"Explanation: \" + df['StudentExplanation_to_encode']\n",
    "    df = extract_numerical_features(df)\n",
    "    # Clean up intermediate columns before returning\n",
    "    df.drop(columns=['QuestionText_N', 'MC_Answer_N', 'StudentExplanation_N',\n",
    "                     'QuestionText_to_encode', 'MC_Answer_to_encode', 'StudentExplanation_to_encode'], \n",
    "            inplace=True)\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "sbert_model = None\n",
    "if CFG.vectorizer_name == 'SentenceBERT':\n",
    "    print(f\"Loading Sentence-BERT model from: {CFG.sbert_model_path}\")\n",
    "    sbert_model = SentenceTransformer(CFG.sbert_model_path)\n",
    "\n",
    "def get_sbert_features(texts, model):\n",
    "    \"\"\"\n",
    "    Generates sentence embeddings using a pre-loaded Sentence-BERT model.\n",
    "    \"\"\"\n",
    "    embeddings = model.encode(texts, show_progress_bar=False)\n",
    "    return np.array(embeddings)\n",
    "    \n",
    "# --- Main Script ---\n",
    "model_dir = Path('/kaggle/input/map-math/MAP_BERT_V2')\n",
    "\n",
    "# 1. Load Data\n",
    "input_dir = Path('/kaggle/input/map-charting-student-math-misunderstandings')\n",
    "test_df = pd.read_csv(input_dir / 'test.csv')\n",
    "# Load encoders\n",
    "label_encoder = joblib.load(model_dir / 'label_encoder.joblib')\n",
    "cat_encoder = joblib.load(model_dir / 'cat_encoder.joblib')\n",
    "\n",
    "# Display columns before feature engineering\n",
    "print(\"Test columns BEFORE feature engineering:\", list(test_df.columns))\n",
    "\n",
    "# 2. Feature Engineering and Label Creation\n",
    "print(\"Creating and cleaning text features...\")\n",
    "test_df = create_features_corrected(test_df)\n",
    "\n",
    "# Display columns after feature engineering\n",
    "print(\"Test columns AFTER feature engineering:\", list(test_df.columns))\n",
    "\n",
    "# Display number of features added\n",
    "num_added_test = len(test_df.columns) - len(pd.read_csv(input_dir / 'test.csv').columns)\n",
    "print(f\"Number of features added to test_df: {num_added_test}\")\n",
    "\n",
    "# Display value counts for key engineered features\n",
    "feature_cols = ['QuestionId', 'math_topic', 'question_type', 'has_image', 'question_num_count', 'answer_has_num', 'answer_num_in_question']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Save vectorizer after fitting ---\n",
    "# --- Precompute all features outside the CV loop ---\n",
    "\n",
    "# Create embeddings for the problem context\n",
    "X_context_test = get_sbert_features(test_df['problem_context'], sbert_model)\n",
    "# Create embeddings for the explanation\n",
    "X_explanation_test = get_sbert_features(test_df['explanation_text'], sbert_model)\n",
    "\n",
    "\n",
    "# Include both 'QuestionId' and 'math_topic' in the categorical encoding\n",
    "columns_to_encode = ['QuestionId', 'math_topic', 'question_type', 'has_image', 'question_num_count', 'answer_has_num', 'answer_num_in_question']\n",
    "print(\"columns_to_encode:\", columns_to_encode)\n",
    "X_cat_test = cat_encoder.transform(test_df[columns_to_encode])\n",
    "\n",
    "def get_row_slice(X, idx):\n",
    "    return X[idx, :]\n",
    "\n",
    "def load_and_predict(test_df):\n",
    "    \"\"\"\n",
    "    Loads all saved objects and makes predictions for the given test_df using all fold models (ensemble).\n",
    "    Returns:\n",
    "        DataFrame with predictions (top 3 labels for each row)\n",
    "    \"\"\"\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from pathlib import Path\n",
    "\n",
    "    model_dir = Path('/kaggle/input/map-math/MAP_BERT_V2')\n",
    "\n",
    "    # Load encoders\n",
    "    label_encoder = joblib.load(model_dir / 'label_encoder.joblib')\n",
    "    cat_encoder = joblib.load(model_dir / 'cat_encoder.joblib')\n",
    "\n",
    "    # Recreate all engineered features (must match training)\n",
    "    test_df = create_features_corrected(test_df)\n",
    "\n",
    "    # Load SentenceBERT model\n",
    "    sbert_model = SentenceTransformer(CFG.sbert_model_path)\n",
    "    # Create embeddings for the problem context and explanation\n",
    "    X_context_test = get_sbert_features(test_df['problem_context'], sbert_model)\n",
    "    X_explanation_test = get_sbert_features(test_df['explanation_text'], sbert_model)\n",
    "\n",
    "    # Categorical features (must match training columns)\n",
    "    columns_to_encode = ['QuestionId', 'math_topic', 'question_type', 'has_image', 'question_num_count', 'answer_has_num', 'answer_num_in_question']\n",
    "    X_cat_test = cat_encoder.transform(test_df[columns_to_encode])\n",
    "\n",
    "    # Combine all features\n",
    "    X_test_all = np.hstack([X_context_test, X_explanation_test, X_cat_test])\n",
    "\n",
    "    # Ensemble predictions from all folds\n",
    "    n_folds = 10\n",
    "    n_classes = len(label_encoder.classes_)\n",
    "    test_proba = np.zeros((X_test_all.shape[0], n_classes))\n",
    "    for fold in range(1, n_folds + 1):\n",
    "        model = joblib.load(model_dir / f'{CFG.model_name}_fold{fold}.joblib')\n",
    "        test_proba += model.predict_proba(X_test_all) / n_folds\n",
    "\n",
    "    return test_proba\n",
    "\n",
    "test_predictions = load_and_predict(test_df)\n",
    "\n",
    "# Submission\n",
    "top3_indices = np.argsort(test_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "top3_labels = label_encoder.inverse_transform(top3_indices.flatten()).reshape(top3_indices.shape)\n",
    "\n",
    "print(\"Saving ranked probabilities for the XGBoost model...\")\n",
    "\n",
    "# Get top 25 predictions (indices, labels, and their probabilities)\n",
    "top_indices_full = np.argsort(test_predictions, axis=1)[:, ::-1][:, :25]\n",
    "top_labels_full = label_encoder.inverse_transform(top_indices_full.flatten()).reshape(top_indices_full.shape)\n",
    "\n",
    "# Get the probability values corresponding to the top predictions\n",
    "top_probs_full = np.array([test_predictions[i, top_indices_full[i]] for i in range(len(test_predictions))])\n",
    "\n",
    "prob_data = []\n",
    "for i in range(len(test_df)):\n",
    "    prob_dict = {f\"prob_{j}\": top_probs_full[i, j] for j in range(25)}\n",
    "    prob_dict['row_id'] = test_df.row_id.values[i]\n",
    "    prob_dict['top_classes'] = \" \".join(top_labels_full[i, :25])\n",
    "    prob_data.append(prob_dict)\n",
    "\n",
    "prob_df = pd.DataFrame(prob_data)\n",
    "prob_df.to_csv(\"submission_xgboost_probabilities.csv\", index=False)\n",
    "\n",
    "print(\"XGBoost probability file created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb19047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T01:48:38.493234Z",
     "iopub.status.busy": "2025-10-09T01:48:38.492977Z",
     "iopub.status.idle": "2025-10-09T01:57:51.990352Z",
     "shell.execute_reply": "2025-10-09T01:57:51.989086Z"
    },
    "papermill": {
     "duration": 553.505333,
     "end_time": "2025-10-09T01:57:51.992376",
     "exception": false,
     "start_time": "2025-10-09T01:48:38.487043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-09 01:48:50.024466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1759974530.232008      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1759974530.293990      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 10-09 01:49:02 [__init__.py:244] Automatically detected platform cuda.\r\n",
      "[INFO] ====== Qwen-SFT vLLM Inference (Hybrid) ======\r\n",
      "[INFO] train=(36696, 7), test=(3, 5)\r\n",
      "[WARN] Some label indices are multi-token -> Fallback to text-label extraction (Case B)\r\n",
      "[INFO] GPU count=2, tensor_parallel_size=2\r\n",
      "INFO 10-09 01:49:22 [config.py:841] This model supports multiple tasks: {'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.\r\n",
      "WARNING 10-09 01:49:22 [config.py:3371] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 10-09 01:49:22 [config.py:1472] Using max model len 256\r\n",
      "WARNING 10-09 01:49:22 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "WARNING 10-09 01:49:23 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 10-09 01:49:23 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233', speculative_config=None, tokenizer='/kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=256, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=/kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "WARNING 10-09 01:49:24 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 10-09 01:49:24 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 10-09 01:49:24 [cuda.py:360] Using XFormers backend.\r\n",
      "2025-10-09 01:49:30.769717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1759974570.805752      73 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1759974570.816296      73 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 10-09 01:49:34 [__init__.py:244] Automatically detected platform cuda.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:49:37 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:49:37 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:49:37 [cuda.py:360] Using XFormers backend.\r\n",
      "[W1009 01:49:48.551867334 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1009 01:49:48.942588498 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1009 01:49:58.562731916 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1009 01:50:08.573259069 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 10-09 01:50:08 [__init__.py:1152] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:50:08 [__init__.py:1152] Found nccl from library libnccl.so.2\r\n",
      "INFO 10-09 01:50:08 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:50:08 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 10-09 01:50:08 [custom_all_reduce_utils.py:208] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 10-09 01:50:34 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:50:34 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 10-09 01:50:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_a0c2b07b'), local_subscribe_addr='ipc:///tmp/a80ff644-2e2f-4be2-a88a-6b245bf7ca24', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "INFO 10-09 01:50:34 [parallel_state.py:1076] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:50:34 [parallel_state.py:1076] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\r\n",
      "INFO 10-09 01:50:34 [model_runner.py:1171] Starting to load model /kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:50:35 [model_runner.py:1171] Starting to load model /kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:44<02:12, 44.28s/it]\r\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [01:21<01:20, 40.16s/it]\r\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:31<00:26, 26.31s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:13<00:00, 32.43s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:13<00:00, 33.30s/it]\r\n",
      "\r\n",
      "INFO 10-09 01:52:48 [default_loader.py:272] Loading weights took 133.30 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:52:48 [default_loader.py:272] Loading weights took 133.13 seconds\r\n",
      "INFO 10-09 01:52:49 [model_runner.py:1203] Model loading took 7.6369 GiB and 133.578371 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:52:49 [model_runner.py:1203] Model loading took 7.6369 GiB and 133.405342 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:52:57 [worker.py:294] model weights take 7.64GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.16GiB; the rest of the memory reserved for KV Cache is 6.09GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 10-09 01:52:58 [worker.py:294] model weights take 7.64GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 4.84GiB.\r\n",
      "INFO 10-09 01:52:58 [executor_base.py:113] # cuda blocks: 4409, # CPU blocks: 3640\r\n",
      "INFO 10-09 01:52:58 [executor_base.py:118] Maximum concurrency for 256 tokens per request: 275.56x\r\n",
      "INFO 10-09 01:53:04 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 14.21 seconds\r\n",
      "[INFO] Generating (Case B)...\r\n",
      "Adding requests: 100%|███████████████████████████| 3/3 [00:00<00:00, 990.62it/s]\r\n",
      "Processed prompts: 100%|█| 3/3 [00:01<00:00,  2.08it/s, est. speed input: 271.51\r\n",
      "[INFO] Generation done in 1.4s\r\n",
      "[INFO] Saved probabilities to /kaggle/working/submission_qwen_sft_probabilities.csv\r\n",
      "[INFO] Saved submission to   /kaggle/working/submission_qwen_sft.csv\r\n",
      "[INFO] Preview probabilities:\r\n",
      "   row_id  prob_0  prob_1  prob_2  ...  prob_22  prob_23  prob_24  top_classes\r\n",
      "0   36696     0.0     0.0     0.0  ...      0.0      0.0      0.0             \r\n",
      "1   36697     0.0     0.0     0.0  ...      0.0      0.0      0.0             \r\n",
      "2   36698     0.0     0.0     0.0  ...      0.0      0.0      0.0             \r\n",
      "\r\n",
      "[3 rows x 27 columns]\r\n",
      "INFO 10-09 01:53:05 [multiproc_worker_utils.py:138] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-09 01:53:05 [multiproc_worker_utils.py:260] Worker exiting\r\n",
      "[INFO] Done.\r\n",
      "GPU count: 2\n",
      "=== GPU 0 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "=== GPU 1 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "2025-10-09 01:53:26.741406: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1759974806.768392     262 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1759974806.776703     262 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Train shape: (36696, 9) with 65 target classes\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:24<00:00, 21.23s/it]\r\n",
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma2-9b-it-bf16 and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\r\n",
      "  warnings.warn(\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 113.61 examples/s]\r\n",
      "Inference:   0%|                                          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\r\n",
      "  warnings.warn(\r\n",
      "Inference: 100%|██████████████████████████████████| 1/1 [00:02<00:00,  2.69s/it]\r\n",
      "GPU count: 2\n",
      "=== GPU 0 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "=== GPU 1 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "2025-10-09 01:55:21.493152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1759974921.520167     289 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1759974921.528027     289 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Train shape: (36696, 9) with 65 target classes\r\n",
      " Starting multi-GPU inference...\r\n",
      "Loading deepseek on cuda:0...\r\n",
      "Loading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]Loading qwen3 on cuda:1...\r\n",
      "\r\n",
      "Loading checkpoint shards:  33%|██████            | 1/3 [00:41<01:22, 41.49s/it]\r\n",
      "Loading checkpoint shards:  67%|████████████      | 2/3 [01:26<00:43, 43.83s/it]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [01:50<00:00, 36.93s/it]\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 123.57 examples/s]\r\n",
      "deepseek: 100%|███████████████████████████████████| 1/1 [00:01<00:00,  1.25s/it]\r\n",
      " deepseek completed - saved submission and probabilities\r\n",
      "\r\n",
      "Loading checkpoint shards:  75%|█████████████▌    | 3/4 [02:04<00:41, 41.07s/it]\u001b[A\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [02:07<00:00, 31.81s/it]\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 621.01 examples/s]\r\n",
      "qwen3: 100%|██████████████████████████████████████| 1/1 [00:00<00:00,  2.81it/s]\r\n",
      " qwen3 completed - saved submission and probabilities\r\n",
      " completed in 138.58 seconds!\r\n",
      "GPU count: 2\n",
      "=== GPU 0 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "=== GPU 1 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# vLLM (Qwen SFT, TP=2 on GPUs 0,1)\n",
    "!python /kaggle/working/vllm_qwen_sft_inference.py\n",
    "time.sleep(5)\n",
    "print_gpu_memory()\n",
    "\n",
    "# 以降は今まで通り\n",
    "!python /kaggle/working/gemma2_inference.py\n",
    "time.sleep(5)\n",
    "print_gpu_memory()\n",
    "\n",
    "!python /kaggle/working/qwen3_deepseek_inference.py\n",
    "time.sleep(5)\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8b22a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T05:24:19.923503Z",
     "iopub.status.busy": "2025-09-05T05:24:19.922686Z",
     "iopub.status.idle": "2025-09-05T05:24:19.927662Z",
     "shell.execute_reply": "2025-09-05T05:24:19.926846Z",
     "shell.execute_reply.started": "2025-09-05T05:24:19.923473Z"
    },
    "papermill": {
     "duration": 0.008604,
     "end_time": "2025-10-09T01:57:52.010773",
     "exception": false,
     "start_time": "2025-10-09T01:57:52.002169",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd004bf4",
   "metadata": {
    "papermill": {
     "duration": 0.008233,
     "end_time": "2025-10-09T01:57:52.027353",
     "exception": false,
     "start_time": "2025-10-09T01:57:52.019120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14167351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T01:57:52.048127Z",
     "iopub.status.busy": "2025-10-09T01:57:52.047812Z",
     "iopub.status.idle": "2025-10-09T01:57:52.572325Z",
     "shell.execute_reply": "2025-10-09T01:57:52.571277Z"
    },
    "papermill": {
     "duration": 0.53771,
     "end_time": "2025-10-09T01:57:52.573740",
     "exception": false,
     "start_time": "2025-10-09T01:57:52.036030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved submission.csv\n",
      "   row_id                             Category:Misconception\n",
      "0   36696  True_Correct:NA True_Neither:NA True_Misconcep...\n",
      "1   36697  False_Misconception:WNB False_Neither:NA False...\n",
      "2   36698  True_Neither:NA True_Correct:NA True_Misconcep...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MAP - Ensemble with Disagreement Handling (Robust Version)\n",
    "# ------------------------------------------------------------\n",
    "# - 安全なCSV読み込み（top_classesのNaN→\"\"、prob_*のNaN→0.0）\n",
    "# - 行ごと抽出でもNaN安全：.get と型チェックでsplit()/float変換を保護\n",
    "# - 重み(model_weights)がNone/不足/過剰でも自動調整\n",
    "# - すべてのモデルでクラスが空でも安全（空文字を返す）\n",
    "# - デバッグプリント（最初の数行）\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def _load_prob_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"probabilities CSV を安全に読み込んで整形\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # row_id の型をそろえる\n",
    "    if \"row_id\" not in df.columns:\n",
    "        raise KeyError(f\"'row_id' column is missing in {path}\")\n",
    "    df[\"row_id\"] = df[\"row_id\"].astype(int)\n",
    "\n",
    "    # top_classes* を安全化（NaN -> \"\"、非文字列 -> str）\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"top_classes\"):\n",
    "            df[c] = df[c].fillna(\"\").astype(str)\n",
    "\n",
    "    # prob_* を安全化（NaN -> 0.0、非数 -> 0.0）\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"prob_\"):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_class_probabilities(row: pd.Series, model_suffix: str = \"\", top_k: int = 25) -> dict:\n",
    "    \"\"\"\n",
    "    行からクラス名と確率を抽出（NaN/欠損に強い）\n",
    "    返り値: { class_name(str): prob(float), ... }\n",
    "    \"\"\"\n",
    "    classes_col = f\"top_classes{model_suffix}\"\n",
    "    raw = row.get(classes_col, \"\")\n",
    "    if isinstance(raw, float) and np.isnan(raw):\n",
    "        classes = []\n",
    "    else:\n",
    "        raw = \"\" if raw is None else str(raw)\n",
    "        classes = raw.split()[:top_k] if raw else []\n",
    "\n",
    "    class_probs = {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        prob_col = f\"prob_{i}{model_suffix}\"\n",
    "        p = row.get(prob_col, 0.0)\n",
    "        try:\n",
    "            p = float(p)\n",
    "        except Exception:\n",
    "            p = 0.0\n",
    "        class_probs[cls] = p\n",
    "    return class_probs\n",
    "\n",
    "\n",
    "def _normalize_weights(n_models: int, model_weights):\n",
    "    \"\"\"重みを n_models に合わせて安全化（None→等重み、不足→後ろを1.0、過剰→切り落とし）\"\"\"\n",
    "    if model_weights is None:\n",
    "        return [1.0] * n_models\n",
    "    w = list(model_weights)\n",
    "    if len(w) < n_models:\n",
    "        w = w + [1.0] * (n_models - len(w))\n",
    "    elif len(w) > n_models:\n",
    "        w = w[:n_models]\n",
    "    return w\n",
    "\n",
    "\n",
    "def ensemble_with_disagreement_handling(prob_files, model_weights=None, top_k=3, debug=False):\n",
    "    \"\"\"\n",
    "    複数の probability CSV を row_id でマージし、重み付き合意/不一致処理で上位 top_k を返す。\n",
    "    戻り値: List[str]（各rowの \"class1 class2 class3\"）\n",
    "    \"\"\"\n",
    "    n_models = len(prob_files)\n",
    "    weights = _normalize_weights(n_models, model_weights)\n",
    "\n",
    "    # 個別読込（安全化）\n",
    "    prob_dfs = [_load_prob_csv(p) for p in prob_files]\n",
    "\n",
    "    # マージ（suffixesで列名衝突を回避）\n",
    "    merged_df = prob_dfs[0]\n",
    "    for i, df in enumerate(prob_dfs[1:], 1):\n",
    "        merged_df = pd.merge(\n",
    "            merged_df, df, on=\"row_id\", how=\"inner\",\n",
    "            suffixes=(\"\", f\"_model{i+1}\")\n",
    "        )\n",
    "\n",
    "    if debug:\n",
    "        print(\"[DEBUG] Merged columns:\", list(merged_df.columns)[:20], \"...\")\n",
    "        print(\"[DEBUG] Merged head:\\n\", merged_df.head())\n",
    "\n",
    "    final_predictions = []\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "        # 各モデルから {class: prob} を抽出\n",
    "        all_class_probs = []\n",
    "        for i in range(n_models):\n",
    "            suffix = f\"_model{i+1}\" if i > 0 else \"\"\n",
    "            class_probs = extract_class_probabilities(row, suffix, top_k=25)\n",
    "            all_class_probs.append(class_probs)\n",
    "\n",
    "        # クラス集合\n",
    "        all_classes = set()\n",
    "        for class_probs in all_class_probs:\n",
    "            all_classes.update(class_probs.keys())\n",
    "\n",
    "        if not all_classes:\n",
    "            # どのモデルもクラスを出していない場合のフォールバック（空文字1つ）\n",
    "            final_predictions.append(\"\")\n",
    "            continue\n",
    "\n",
    "        # 投票・合計・最大確率（重み付き）\n",
    "        class_votes = defaultdict(int)\n",
    "        class_total_prob = defaultdict(float)\n",
    "        class_max_prob = defaultdict(float)\n",
    "\n",
    "        for i, class_probs in enumerate(all_class_probs):\n",
    "            weight = weights[i]\n",
    "            for cls, prob in class_probs.items():\n",
    "                class_votes[cls] += 1\n",
    "                class_total_prob[cls] += prob * weight\n",
    "                class_max_prob[cls] = max(class_max_prob[cls], prob * weight)\n",
    "\n",
    "        # スコア計算\n",
    "        final_scores = {}\n",
    "        n_models_float = float(n_models)\n",
    "        for cls in all_classes:\n",
    "            base = class_total_prob[cls]                 # 60%\n",
    "            agreement = class_votes[cls] / n_models_float  # 30%\n",
    "            confidence = class_max_prob[cls]             # 10%\n",
    "            final_scores[cls] = base * 0.6 + agreement * 0.3 + confidence * 0.1\n",
    "\n",
    "        # ソートして上位 top_k\n",
    "        sorted_classes = sorted(final_scores.items(), key=lambda x: -x[1])\n",
    "        top_classes = [cls for cls, _ in sorted_classes[:top_k]]\n",
    "\n",
    "        final_predictions.append(\" \".join(top_classes))\n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "\n",
    "# ====== 設定 ======\n",
    "# 単体モデル参考スコア（コメント）\n",
    "# deepseek math 7b - 0.944\n",
    "# qwen3 8b        - 0.943\n",
    "# gemma 2 9b      - 0.942\n",
    "w1, w2, w3, w4, w5 = 1.5, 1.0, 0.5, 1.0, 1.0\n",
    "\n",
    "prob_files = [\n",
    "    \"/kaggle/working/submission_deepseek_probabilities.csv\",\n",
    "    \"/kaggle/working/submission_qwen3_probabilities.csv\",\n",
    "    \"/kaggle/working/submission_qwen_sft_probabilities.csv\",  # vLLM SFT\n",
    "    # 追加するならこちらへ（例：ResNet/XGBoost）\n",
    "    # \"/kaggle/working/submission_resnet_probabilities.csv\",\n",
    "    # \"/kaggle/working/submission_xgboost_probabilities.csv\",\n",
    "]\n",
    "\n",
    "# 読み込むファイルの数に合わせて重みを自動調整します（不足→1.0追加／過剰→切り捨て）\n",
    "predictions = ensemble_with_disagreement_handling(\n",
    "    prob_files,\n",
    "    model_weights=[w1, w2, w3, w4, w5],  # ファイル数に合わせて内部で整形\n",
    "    top_k=3,\n",
    "    debug=False,  # Trueにするとマージ後headを表示\n",
    ")\n",
    "\n",
    "# 提出CSV\n",
    "test_df = pd.read_csv(\"/kaggle/input/map-charting-student-math-misunderstandings/test.csv\")\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": test_df.row_id.values.astype(int),\n",
    "    \"Category:Misconception\": predictions\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"[INFO] Saved submission.csv\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "isSourceIdPinned": false,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "datasetId": 2889918,
     "sourceId": 4982782,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7930680,
     "sourceId": 12559632,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7930694,
     "sourceId": 12559652,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8039184,
     "sourceId": 12719174,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8045877,
     "sourceId": 12729471,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8278373,
     "sourceId": 13071570,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8381858,
     "sourceId": 13241585,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 261755536,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 778.307421,
   "end_time": "2025-10-09T01:57:53.605116",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-09T01:44:55.297695",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
