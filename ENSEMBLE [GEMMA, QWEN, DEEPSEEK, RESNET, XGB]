{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76ebd1c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003367,
     "end_time": "2025-10-12T12:57:26.039229",
     "exception": false,
     "start_time": "2025-10-12T12:57:26.035862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ensemble Gemma-2 9b, Qwen3 8b & Deepseek math 7b \n",
    "\n",
    "Inference runs in 2hours, so lot of room for more models. We infer Gemma 9b on 2 gpus because it is loaded in fp16. We run Qwen 3 8b and deepseek math 7b parallel on 2 gpus, to save time. After inference, for ensembling we use prob confidence, weighted average and agreement between models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Credits:   \n",
    "@cdeotte - [Gemma 9b weights](https://www.kaggle.com/datasets/cdeotte/gemma2-9b-it-cv945)  \n",
    "@jaytonde - [Qwen 3 8b weights](https://www.kaggle.com/datasets/jaytonde/qwen3-8b-map-competition)  \n",
    "@jaytonde - [Deepseek math 7b weights](https://www.kaggle.com/datasets/jaytonde/deekseepmath-7b-map-competition) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5850059d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T12:57:26.045674Z",
     "iopub.status.busy": "2025-10-12T12:57:26.045426Z",
     "iopub.status.idle": "2025-10-12T13:00:18.114515Z",
     "shell.execute_reply": "2025-10-12T13:00:18.113851Z"
    },
    "papermill": {
     "duration": 172.073708,
     "end_time": "2025-10-12T13:00:18.115898",
     "exception": false,
     "start_time": "2025-10-12T12:57:26.042190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "cudf-cu12 25.2.2 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\r\n",
      "ydata-profiling 4.16.1 requires numba<=0.61,>=0.56.0, but you have numba 0.61.2 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\r\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\r\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qq --no-index --find-links=/kaggle/input/map-library-public \\ vllm==\"0.9.2\"\n",
    "# check \n",
    "import vllm \n",
    "import transformers \n",
    "import torch \n",
    "assert vllm.__version__ == \"0.9.2\" \n",
    "assert transformers.__version__ == \"4.52.4\" \n",
    "assert torch.__version__ == \"2.7.0+cu126\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f740ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T13:00:18.122970Z",
     "iopub.status.busy": "2025-10-12T13:00:18.122667Z",
     "iopub.status.idle": "2025-10-12T13:00:18.245198Z",
     "shell.execute_reply": "2025-10-12T13:00:18.244371Z"
    },
    "papermill": {
     "duration": 0.127367,
     "end_time": "2025-10-12T13:00:18.246458",
     "exception": false,
     "start_time": "2025-10-12T13:00:18.119091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU count: 2\n",
      "=== GPU 0 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "=== GPU 1 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"現在の GPU メモリ使用状況を表示する\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available.\")\n",
    "        return\n",
    "    \n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(\"GPU count:\", gpu_count)\n",
    "\n",
    "    for i in range(gpu_count):\n",
    "        print(f\"=== GPU {i} ===\")\n",
    "        print(\"Name:\", torch.cuda.get_device_name(i))\n",
    "        print(\"Memory Allocated:\", round(torch.cuda.memory_allocated(i)/1024**2), \"MB\")\n",
    "        print(\"Memory Reserved: \", round(torch.cuda.memory_reserved(i)/1024**2), \"MB\")\n",
    "        print(\"Max Allocated:   \", round(torch.cuda.max_memory_allocated(i)/1024**2), \"MB\")\n",
    "        print(\"Max Reserved:    \", round(torch.cuda.max_memory_reserved(i)/1024**2), \"MB\")\n",
    "        print()\n",
    "\n",
    "# 使い方:\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5d9a94",
   "metadata": {
    "papermill": {
     "duration": 0.002889,
     "end_time": "2025-10-12T13:00:18.252626",
     "exception": false,
     "start_time": "2025-10-12T13:00:18.249737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model-1: Gemma2  9b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7bc8b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T13:00:18.260840Z",
     "iopub.status.busy": "2025-10-12T13:00:18.260580Z",
     "iopub.status.idle": "2025-10-12T13:00:18.267071Z",
     "shell.execute_reply": "2025-10-12T13:00:18.266485Z"
    },
    "papermill": {
     "duration": 0.012302,
     "end_time": "2025-10-12T13:00:18.268114",
     "exception": false,
     "start_time": "2025-10-12T13:00:18.255812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing gemma2_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gemma2_inference.py\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import os\n",
    "from IPython.display import display, Math, Latex\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from peft import PeftModel\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "lora_path = \"/kaggle/input/gemma2-9b-it-cv945\"\n",
    "MAX_LEN = 256\n",
    "# helpers\n",
    "def format_input(row):\n",
    "    x = \"Yes\"\n",
    "    if not row['is_correct']:\n",
    "        x = \"No\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"Correct? {x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\"\n",
    "    )\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "\n",
    "train.Misconception = train.Misconception.fillna('NA')\n",
    "train['target'] = train.Category+\":\"+train.Misconception\n",
    "train['label'] = le.fit_transform(train['target'])\n",
    "target_classes = le.classes_\n",
    "n_classes = len(target_classes)\n",
    "print(f\"Train shape: {train.shape} with {n_classes} target classes\")\n",
    "idx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\n",
    "correct = train.loc[idx].copy()\n",
    "correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "correct = correct.sort_values('c',ascending=False)\n",
    "correct = correct.drop_duplicates(['QuestionId'])\n",
    "correct = correct[['QuestionId','MC_Answer']]\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "# Prepare test data\n",
    "test = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "test.is_correct = test.is_correct.fillna(0)\n",
    "test['text'] = test.apply(format_input, axis=1)\n",
    "\n",
    "# Create a directory for model offloading\n",
    "os.makedirs(\"offload\", exist_ok=True)\n",
    "\n",
    "# load model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"/kaggle/input/gemma2-9b-it-bf16\",\n",
    "    num_labels=n_classes,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    # offload_folder=\"offload\",  # <-- Add this line\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "model.eval()\n",
    "\n",
    "# Tokenize dataset\n",
    "ds_test = Dataset.from_pandas(test[['text']])\n",
    "ds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n",
    "\n",
    "# Create data collator for efficient batching with padding\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=MAX_LEN,  \n",
    "    return_tensors=\"pt\")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ds_test,\n",
    "    batch_size=8,  \n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    "    pin_memory=True,  \n",
    "    num_workers=2     \n",
    ")\n",
    "\n",
    "# Fast inference loop\n",
    "all_logits = []\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "        # Move batch to device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Convert bfloat16 to float32 then move to CPU and store\n",
    "        all_logits.append(logits.float().cpu().numpy())\n",
    "\n",
    "# Concatenate all logits\n",
    "predictions = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "# Convert to probs\n",
    "probs = softmax(predictions, axis=1)\n",
    "\n",
    "# Get top predictions (all 65 classes ranked)\n",
    "top_indices = np.argsort(-probs, axis=1)\n",
    "\n",
    "# Decode to class names\n",
    "flat_indices = top_indices.flatten()\n",
    "decoded_labels = le.inverse_transform(flat_indices)\n",
    "top_labels = decoded_labels.reshape(top_indices.shape)\n",
    "\n",
    "# Create submission (top 3)\n",
    "joined_preds = [\" \".join(row[:3]) for row in top_labels]\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    \"row_id\": test.row_id.values,\n",
    "    \"Category:Misconception\": joined_preds\n",
    "})\n",
    "sub.to_csv(\"submission_gemma.csv\", index=False)\n",
    "\n",
    "prob_data = []\n",
    "for i in range(len(test)):\n",
    "    prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(25)}  # Top 25\n",
    "    prob_dict['row_id'] = test.row_id.values[i]\n",
    "    prob_dict['top_classes'] = \" \".join(top_labels[i, :25])  # Top 25 class names\n",
    "    prob_data.append(prob_dict)\n",
    "\n",
    "prob_df = pd.DataFrame(prob_data)\n",
    "prob_df.to_csv(\"submission_gemma_prob.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebd8ed",
   "metadata": {
    "papermill": {
     "duration": 0.002781,
     "end_time": "2025-10-12T13:00:18.273970",
     "exception": false,
     "start_time": "2025-10-12T13:00:18.271189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model 2-3: Qwen 3 8b & Deepseek math 7b parallel\n",
    "Run deepseek on cuda:0 and qwen 3 on cuda:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2d5fa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T13:00:18.280925Z",
     "iopub.status.busy": "2025-10-12T13:00:18.280596Z",
     "iopub.status.idle": "2025-10-12T13:00:18.286509Z",
     "shell.execute_reply": "2025-10-12T13:00:18.285816Z"
    },
    "papermill": {
     "duration": 0.010819,
     "end_time": "2025-10-12T13:00:18.287585",
     "exception": false,
     "start_time": "2025-10-12T13:00:18.276766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qwen3_deepseek_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qwen3_deepseek_inference.py\n",
    "\n",
    "# we do parallel inference, for deepseek and qwen3\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "import threading\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "\n",
    "model_paths = [\n",
    "    \"/kaggle/input/deekseepmath-7b-map-competition/MAP_EXP_09_FULL\",\n",
    "   \"/kaggle/input/qwen3-8b-map-competition/MAP_EXP_16_FULL\"]\n",
    "\n",
    "def format_input(row):\n",
    "    x = \"This answer is correct.\"\n",
    "    if not row['is_correct']:\n",
    "        x = \"This is answer is incorrect.\"\n",
    "    return (\n",
    "        f\"Question: {row['QuestionText']}\\n\"\n",
    "        f\"Answer: {row['MC_Answer']}\\n\"\n",
    "        f\"{x}\\n\"\n",
    "        f\"Student Explanation: {row['StudentExplanation']}\")\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "train.Misconception  = train.Misconception.fillna('NA')\n",
    "train['target']   = train.Category + ':' +train.Misconception\n",
    "train['label']    = le.fit_transform(train['target'])\n",
    "\n",
    "n_classes = len(le.classes_)\n",
    "print(f\"Train shape: {train.shape} with {n_classes} target classes\")\n",
    "idx = train.apply(lambda row: row.Category.split('_')[0],axis=1)=='True'\n",
    "correct = train.loc[idx].copy()\n",
    "correct['c'] = correct.groupby(['QuestionId','MC_Answer']).MC_Answer.transform('count')\n",
    "correct = correct.sort_values('c',ascending=False)\n",
    "correct = correct.drop_duplicates(['QuestionId'])\n",
    "correct = correct[['QuestionId','MC_Answer']]\n",
    "correct['is_correct'] = 1\n",
    "\n",
    "test = test.merge(correct, on=['QuestionId','MC_Answer'], how='left')\n",
    "test.is_correct = test.is_correct.fillna(0)\n",
    "test['text'] = test.apply(format_input,axis=1)\n",
    "ds_test = Dataset.from_pandas(test)\n",
    "\n",
    "\n",
    "def run_inference_on_gpu(model_path, gpu_id, test_data, output_name):\n",
    "    \"\"\"Run inference for one model on one GPU\"\"\"\n",
    "    \n",
    "    device = f\"cuda:{gpu_id}\"\n",
    "    print(f\"Loading {output_name} on {device}...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, \n",
    "        device_map=device, \n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize function\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch[\"text\"], \n",
    "                        truncation=True,\n",
    "                        max_length=256)\n",
    "    \n",
    "    ds_test = Dataset.from_pandas(test_data[['text']])\n",
    "    ds_test = ds_test.map(tokenize, batched=True, remove_columns=['text'])\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        ds_test,\n",
    "        batch_size=4,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "        pin_memory=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Inference\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=f\"{output_name}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            all_logits.append(outputs.logits.float().cpu().numpy())\n",
    "    \n",
    "    predictions = np.concatenate(all_logits, axis=0)\n",
    "    \n",
    "    # Process results\n",
    "    probs = softmax(predictions, axis=1)\n",
    "    top_indices = np.argsort(-probs, axis=1)\n",
    "    \n",
    "    # Decode labels\n",
    "    flat_indices = top_indices.flatten()\n",
    "    decoded_labels = le.inverse_transform(flat_indices)\n",
    "    top_labels = decoded_labels.reshape(top_indices.shape)\n",
    "    \n",
    "    # Save top-3 submission\n",
    "    joined_preds = [\" \".join(row[:3]) for row in top_labels]\n",
    "    sub = pd.DataFrame({\n",
    "        \"row_id\": test_data.row_id.values,\n",
    "        \"Category:Misconception\": joined_preds\n",
    "    })\n",
    "    sub.to_csv(f\"submission_{output_name}.csv\", index=False)\n",
    "    \n",
    "    # Save probabilities for ensemble\n",
    "    prob_data = []\n",
    "    for i in range(len(predictions)):\n",
    "        prob_dict = {f\"prob_{j}\": probs[i, top_indices[i, j]] for j in range(25)}\n",
    "        prob_dict['row_id'] = test_data.row_id.values[i]\n",
    "        prob_dict['top_classes'] = \" \".join(top_labels[i, :25])\n",
    "        prob_data.append(prob_dict)\n",
    "    \n",
    "    prob_df = pd.DataFrame(prob_data)\n",
    "    prob_df.to_csv(f\"submission_{output_name}_probabilities.csv\", index=False)\n",
    "    \n",
    "    print(f\" {output_name} completed - saved submission and probabilities\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    del model, tokenizer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\" Starting multi-GPU inference...\")\n",
    "start_time = time.time()\n",
    "\n",
    "threads = []\n",
    "gpu_assignments = [\n",
    "    (model_paths[0], 0, \"deepseek\"),\n",
    "    (model_paths[1], 1, \"qwen3\"),\n",
    "]\n",
    "\n",
    "# Start threads\n",
    "for model_path, gpu_id, name in gpu_assignments:\n",
    "    if gpu_id < torch.cuda.device_count():  \n",
    "        thread = threading.Thread(\n",
    "            target=run_inference_on_gpu,\n",
    "            args=(model_path, gpu_id, test, name)\n",
    "        )\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "        time.sleep(10)  # Stagger starts to avoid memory issues\n",
    "\n",
    "# Wait for completion\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\" completed in {end_time - start_time:.2f} seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205a475a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T13:00:18.294603Z",
     "iopub.status.busy": "2025-10-12T13:00:18.294373Z",
     "iopub.status.idle": "2025-10-12T13:00:18.303618Z",
     "shell.execute_reply": "2025-10-12T13:00:18.303003Z"
    },
    "papermill": {
     "duration": 0.014258,
     "end_time": "2025-10-12T13:00:18.304694",
     "exception": false,
     "start_time": "2025-10-12T13:00:18.290436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vllm_qwen_sft_inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vllm_qwen_sft_inference.py\n",
    "import os, json, gc, time, re, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, LogitsProcessor\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# ==============================\n",
    "# パス設定\n",
    "# ==============================\n",
    "VLLM_MODEL_DIR  = \"/kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233\"\n",
    "TRAIN_CSV = \"/kaggle/input/map-charting-student-math-misunderstandings/train.csv\"\n",
    "TEST_CSV  = \"/kaggle/input/map-charting-student-math-misunderstandings/test.csv\"\n",
    "OUT_DIR   = \"/kaggle/working\"\n",
    "\n",
    "# ==============================\n",
    "# 実行設定（安定化）\n",
    "# ==============================\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "os.environ.setdefault(\"NCCL_ASYNC_ERROR_HANDLING\", \"1\")\n",
    "os.environ.setdefault(\"NCCL_SHUTDOWN_TIMEOUT\", \"5\")\n",
    "os.environ.setdefault(\"NCCL_P2P_DISABLE\", \"1\")\n",
    "os.environ.setdefault(\"NCCL_IB_DISABLE\", \"1\")\n",
    "os.environ.setdefault(\"VLLM_WORKER_MULTIPROC_METHOD\", \"spawn\")\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0,1\")\n",
    "\n",
    "# vLLM / モデル実行パラメータ\n",
    "VLLM_TP_SIZE         = 2          # T4x2\n",
    "VLLM_DTYPE           = \"float16\"\n",
    "VLLM_MAX_LEN         = 256\n",
    "VLLM_SEED            = 42\n",
    "VLLM_LOGPROBS_K      = 20         # vLLM上限\n",
    "VLLM_MEM_UTILIZATION = 0.95       # 余裕を持たせつつ高め\n",
    "\n",
    "# 出力制御\n",
    "TOP_K_SAVE  = 25   # CSVに保存する最大候補数（vLLMはlogprobs最大20）\n",
    "TOP_K_SUB   = 3    # 提出用top-k\n",
    "\n",
    "# ==============================\n",
    "# ユーティリティ\n",
    "# ==============================\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def _sync_and_free():\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "            for d in range(torch.cuda.device_count()):\n",
    "                torch.cuda.reset_peak_memory_stats(d)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()\n",
    "\n",
    "def _safe_dist_destroy():\n",
    "    \"\"\"barrierは呼ばない（ハング回避）\"\"\"\n",
    "    try:\n",
    "        import torch.distributed as dist\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            try:\n",
    "                dist.destroy_process_group()\n",
    "            except Exception:\n",
    "                pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ==============================\n",
    "# プロンプト\n",
    "# ==============================\n",
    "PROMPT_EN = \"\"\"\\\n",
    "You are a specialist in identifying the types of misunderstandings that arise from students’ answers to math problems.\n",
    "Based on the information provided below, please determine what kind of misunderstanding the student has.\n",
    "\n",
    "Question: {QuestionText}\n",
    "Answer: {MC_Answer}\n",
    "Correct: {Correct}\n",
    "Student Explanation: {StudentExplanation}\n",
    "\n",
    "Respond with ONLY the label text exactly as in the training set (e.g., \"True_Correct:NA\", \"False_Neither:NA\"). Do not add explanations.\n",
    "\"\"\"\n",
    "\n",
    "def _fmt(row) -> str:\n",
    "    return PROMPT_EN.format(\n",
    "        QuestionText=row[\"QuestionText\"],\n",
    "        MC_Answer=row[\"MC_Answer\"],\n",
    "        Correct=\"Yes\" if row[\"is_correct\"] else \"No\",\n",
    "        StudentExplanation=row[\"StudentExplanation\"],\n",
    "    ) + \" \"  # 末尾スペースで即EOS回避の保険\n",
    "\n",
    "# ===== ラベル正規化/照合 =====\n",
    "DIGIT_PREFIX_RE = re.compile(r\"^\\s*([0-9]{1,4})\")\n",
    "IM_SPECIAL = re.compile(r\"<\\|im_[^|]*\\|>\")  # <|im_end|> など\n",
    "\n",
    "def _norm_label(s: str) -> str:\n",
    "    s = (s or \"\").strip()\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip(\" .,:;\")\n",
    "\n",
    "def _build_label_index(all_completions):\n",
    "    idx = {}\n",
    "    for lab in all_completions:\n",
    "        idx[_norm_label(lab)] = lab\n",
    "        idx[_norm_label(lab.replace(\"_\", \" \"))] = lab\n",
    "        idx[_norm_label(lab.replace(\" \", \"_\"))] = lab\n",
    "    return idx\n",
    "\n",
    "def _pick_label_from_text(text: str, label_index: dict):\n",
    "    cand = _norm_label(text)\n",
    "    if not cand:\n",
    "        return None\n",
    "    if cand in label_index:\n",
    "        return label_index[cand]\n",
    "    # 先頭一致（例: \"True_Correct:NA<eos>\"）\n",
    "    for k in label_index.keys():\n",
    "        if cand.startswith(k):\n",
    "            return label_index[k]\n",
    "    return None\n",
    "\n",
    "# ===== LogitsProcessor（Case A用）=====\n",
    "class LabelOnlyLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, allowed_token_ids):\n",
    "        self.allowed_token_ids = allowed_token_ids\n",
    "    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.Tensor:\n",
    "        mask = torch.full_like(scores, float('-inf'))\n",
    "        if scores.dim() == 1:\n",
    "            mask[self.allowed_token_ids] = 0\n",
    "        elif scores.dim() == 2:\n",
    "            mask[:, self.allowed_token_ids] = 0\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected score dimensions\")\n",
    "        return scores + mask\n",
    "\n",
    "# ==============================\n",
    "# メイン\n",
    "# ==============================\n",
    "def main():\n",
    "    seed_everything(VLLM_SEED)\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(\"[INFO] ====== Qwen-SFT vLLM Inference (Hybrid) ======\")\n",
    "\n",
    "    # --- データ準備 ---\n",
    "    train = pd.read_csv(TRAIN_CSV)\n",
    "    test  = pd.read_csv(TEST_CSV)\n",
    "    print(f\"[INFO] train={train.shape}, test={test.shape}\")\n",
    "\n",
    "    # True_* の最頻回答を is_correct=1\n",
    "    idx_true = train[\"Category\"].str.startswith(\"True\")\n",
    "    correct = train.loc[idx_true].copy()\n",
    "    correct[\"cnt\"] = correct.groupby([\"QuestionId\",\"MC_Answer\"]).MC_Answer.transform(\"count\")\n",
    "    correct = (correct.sort_values(\"cnt\", ascending=False)\n",
    "                     .drop_duplicates([\"QuestionId\"])\n",
    "                     .loc[:, [\"QuestionId\",\"MC_Answer\"]])\n",
    "    correct[\"is_correct\"] = 1\n",
    "    test = test.merge(correct, on=[\"QuestionId\",\"MC_Answer\"], how=\"left\")\n",
    "    test[\"is_correct\"] = test[\"is_correct\"].fillna(0)\n",
    "\n",
    "    prompts = test.apply(_fmt, axis=1).tolist()\n",
    "    row_ids = test.row_id.values.astype(int)\n",
    "\n",
    "    # --- ラベル候補 ---\n",
    "    tokenizer_hf = AutoTokenizer.from_pretrained(VLLM_MODEL_DIR, trust_remote_code=True, local_files_only=True)\n",
    "    with open(os.path.join(VLLM_MODEL_DIR, \"all_completions.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "        all_completions = json.load(f)\n",
    "\n",
    "    # 数字ラベルが単一トークンか検査\n",
    "    single_token_ok = True\n",
    "    allowed_token_ids = []\n",
    "    for i in range(len(all_completions)):\n",
    "        ids = tokenizer_hf.encode(str(i), add_special_tokens=False)\n",
    "        if len(ids) != 1:\n",
    "            single_token_ok = False\n",
    "        else:\n",
    "            allowed_token_ids.append(ids[0])\n",
    "\n",
    "    if single_token_ok:\n",
    "        print(\"[INFO] Label indices are single-token ✅  -> Use LogitsProcessor path (Case A)\")\n",
    "    else:\n",
    "        print(\"[WARN] Some label indices are multi-token -> Fallback to text-label extraction (Case B)\")\n",
    "\n",
    "    # --- vLLM 起動 ---\n",
    "    gpu_cnt = max(1, torch.cuda.device_count())\n",
    "    tp = max(1, min(VLLM_TP_SIZE, gpu_cnt))\n",
    "    print(f\"[INFO] GPU count={gpu_cnt}, tensor_parallel_size={tp}\")\n",
    "\n",
    "    llm = LLM(\n",
    "        model=str(VLLM_MODEL_DIR),\n",
    "        tensor_parallel_size=tp,\n",
    "        dtype=VLLM_DTYPE,\n",
    "        gpu_memory_utilization=VLLM_MEM_UTILIZATION,\n",
    "        enforce_eager=True,\n",
    "        max_model_len=VLLM_MAX_LEN,\n",
    "        seed=VLLM_SEED,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    vtok = llm.llm_engine.tokenizer\n",
    "\n",
    "    prob_rows, topk_names = [], []\n",
    "\n",
    "    if single_token_ok:\n",
    "        # ===== Case A: 1トークン分類（参考コードと同じ流儀） =====\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.0, top_p=1.0, top_k=-1,\n",
    "            max_tokens=1,\n",
    "            logprobs=min(3, VLLM_LOGPROBS_K, TOP_K_SAVE),  # MAP@3 を念頭に\n",
    "            stop_token_ids=[vtok.eos_token_id],\n",
    "            logits_processors=[LabelOnlyLogitsProcessor(allowed_token_ids)],\n",
    "        )\n",
    "        print(\"[INFO] Generating (Case A)...\")\n",
    "        t0 = time.time()\n",
    "        outputs = list(llm.generate(prompts, sampling_params))\n",
    "        print(f\"[INFO] Generation done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "        for i, out in enumerate(outputs):\n",
    "            rec = {\"row_id\": int(row_ids[i])}\n",
    "            names = []\n",
    "            for j in range(TOP_K_SAVE):\n",
    "                rec[f\"prob_{j}\"] = 0.0\n",
    "\n",
    "            if out.outputs:\n",
    "                cand_dict = out.outputs[0].logprobs[0] if out.outputs[0].logprobs else None\n",
    "                cand_list = []\n",
    "                if isinstance(cand_dict, dict):\n",
    "                    # token_id は int なので、上位kをそのまま拾う\n",
    "                    # decode->[数字文字列]->int->all_completions[idx]\n",
    "                    for token_id, obj in cand_dict.items():\n",
    "                        logp = getattr(obj, \"logprob\", None)\n",
    "                        if logp is None:\n",
    "                            try:\n",
    "                                logp = float(obj)\n",
    "                            except Exception:\n",
    "                                continue\n",
    "                        try:\n",
    "                            s = tokenizer_hf.decode([int(token_id)]).strip()\n",
    "                        except Exception:\n",
    "                            s = str(token_id)\n",
    "                        if s.isdigit():\n",
    "                            idx = int(s)\n",
    "                            if 0 <= idx < len(all_completions):\n",
    "                                cand_list.append((all_completions[idx], float(np.exp(logp))))\n",
    "                cand_list.sort(key=lambda x: -x[1])\n",
    "                cand_list = cand_list[:min(TOP_K_SAVE, 20)]\n",
    "                for j in range(len(cand_list)):\n",
    "                    rec[f\"prob_{j}\"] = cand_list[j][1]\n",
    "                    names.append(cand_list[j][0])\n",
    "\n",
    "            rec[\"top_classes\"] = \" \".join(names)\n",
    "            prob_rows.append(rec)\n",
    "            topk_names.append(names[:TOP_K_SUB])\n",
    "\n",
    "    else:\n",
    "        # ===== Case B: 文字ラベル抽出 =====\n",
    "        label_index = _build_label_index(all_completions)\n",
    "        sampling_params = SamplingParams(\n",
    "            temperature=0.0, top_p=1.0, top_k=-1,\n",
    "            max_tokens=6,\n",
    "            ignore_eos=True,\n",
    "            logprobs=min(20, VLLM_LOGPROBS_K, TOP_K_SAVE),\n",
    "            stop=None, stop_token_ids=None,\n",
    "        )\n",
    "        print(\"[INFO] Generating (Case B)...\")\n",
    "        t0 = time.time()\n",
    "        outputs = list(llm.generate(prompts, sampling_params))\n",
    "        print(f\"[INFO] Generation done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "        for i, out in enumerate(outputs):\n",
    "            rec = {\"row_id\": int(row_ids[i])}\n",
    "            names = []\n",
    "            for j in range(TOP_K_SAVE):\n",
    "                rec[f\"prob_{j}\"] = 0.0\n",
    "\n",
    "            picked_label = None\n",
    "            cand_list = []\n",
    "\n",
    "            if out.outputs:\n",
    "                o0 = out.outputs[0]\n",
    "                tok_ids = getattr(o0, \"token_ids\", None) or []\n",
    "                try:\n",
    "                    raw_text = vtok.decode(tok_ids, skip_special_tokens=False)\n",
    "                except Exception:\n",
    "                    raw_text = tokenizer_hf.decode(tok_ids, skip_special_tokens=False)\n",
    "                text_no_im = IM_SPECIAL.sub(\"\", raw_text).strip()\n",
    "\n",
    "                # ① 文字ラベル優先\n",
    "                picked_label = _pick_label_from_text(text_no_im, label_index)\n",
    "\n",
    "                # ② 数字フォールバック\n",
    "                if picked_label is None:\n",
    "                    m = DIGIT_PREFIX_RE.match(text_no_im or \"\")\n",
    "                    if m:\n",
    "                        idx_num = int(m.group(1))\n",
    "                        if 0 <= idx_num < len(all_completions):\n",
    "                            picked_label = all_completions[idx_num]\n",
    "\n",
    "                # ③ さらにダメなら1ステップ目logprobsから候補\n",
    "                if picked_label is None and o0.logprobs:\n",
    "                    cand_dict = o0.logprobs[0]\n",
    "                    if isinstance(cand_dict, dict):\n",
    "                        for token_id, obj in cand_dict.items():\n",
    "                            logp = getattr(obj, \"logprob\", None)\n",
    "                            if logp is None:\n",
    "                                try:\n",
    "                                    logp = float(obj)\n",
    "                                except Exception:\n",
    "                                    continue\n",
    "                            tok_str = getattr(obj, \"decoded_token\", None)\n",
    "                            if tok_str is None:\n",
    "                                try:\n",
    "                                    tid = int(token_id)\n",
    "                                    tok_str = tokenizer_hf.decode([tid])\n",
    "                                except Exception:\n",
    "                                    tok_str = str(token_id)\n",
    "                            tok_str = (tok_str or \"\").strip()\n",
    "                            lab = _pick_label_from_text(tok_str, label_index)\n",
    "                            if lab is not None:\n",
    "                                cand_list.append((lab, float(np.exp(logp))))\n",
    "                            elif tok_str.isdigit():\n",
    "                                idx_num = int(tok_str)\n",
    "                                if 0 <= idx_num < len(all_completions):\n",
    "                                    cand_list.append((all_completions[idx_num], float(np.exp(logp))))\n",
    "                        cand_list.sort(key=lambda x: -x[1])\n",
    "                        cand_list = cand_list[:min(TOP_K_SAVE, 20)]\n",
    "\n",
    "            if picked_label is not None:\n",
    "                names.append(picked_label)\n",
    "                rec[\"prob_0\"] = 1.0\n",
    "            else:\n",
    "                for j in range(len(cand_list)):\n",
    "                    rec[f\"prob_{j}\"] = cand_list[j][1]\n",
    "                    names.append(cand_list[j][0])\n",
    "\n",
    "            rec[\"top_classes\"] = \" \".join(names)\n",
    "            prob_rows.append(rec)\n",
    "            topk_names.append(names[:TOP_K_SUB])\n",
    "\n",
    "    # --- 保存 ---\n",
    "    prob_df = pd.DataFrame(prob_rows)\n",
    "    sub_df = pd.DataFrame({\n",
    "        \"row_id\": row_ids,\n",
    "        \"Category:Misconception\": [\" \".join(x) for x in topk_names]\n",
    "    })\n",
    "    prob_path = os.path.join(OUT_DIR, \"submission_qwen_sft_probabilities.csv\")\n",
    "    sub_path  = os.path.join(OUT_DIR, \"submission_qwen_sft.csv\")\n",
    "    prob_df.to_csv(prob_path, index=False)\n",
    "    sub_df.to_csv(sub_path, index=False)\n",
    "    print(f\"[INFO] Saved probabilities to {prob_path}\")\n",
    "    print(f\"[INFO] Saved submission to   {sub_path}\")\n",
    "    print(\"[INFO] Preview probabilities:\")\n",
    "    print(prob_df.head())\n",
    "\n",
    "    # --- 後片付け ---\n",
    "    try:\n",
    "        del llm, tokenizer_hf, prob_rows, topk_names, prompts, row_ids, all_completions\n",
    "    except Exception:\n",
    "        pass\n",
    "    _safe_dist_destroy()\n",
    "    _sync_and_free()\n",
    "    print(\"[INFO] Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0ab449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T13:00:18.311624Z",
     "iopub.status.busy": "2025-10-12T13:00:18.311387Z",
     "iopub.status.idle": "2025-10-12T13:08:52.445567Z",
     "shell.execute_reply": "2025-10-12T13:08:52.444415Z"
    },
    "papermill": {
     "duration": 514.139019,
     "end_time": "2025-10-12T13:08:52.446863",
     "exception": false,
     "start_time": "2025-10-12T13:00:18.307844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-12 13:00:28.874678: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760274029.090858      45 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1760274029.149885      45 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 10-12 13:00:40 [__init__.py:244] Automatically detected platform cuda.\r\n",
      "[INFO] ====== Qwen-SFT vLLM Inference (Hybrid) ======\r\n",
      "[INFO] train=(36696, 7), test=(3, 5)\r\n",
      "[WARN] Some label indices are multi-token -> Fallback to text-label extraction (Case B)\r\n",
      "[INFO] GPU count=2, tensor_parallel_size=2\r\n",
      "INFO 10-12 13:00:57 [config.py:841] This model supports multiple tasks: {'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.\r\n",
      "WARNING 10-12 13:00:57 [config.py:3371] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 10-12 13:00:57 [config.py:1472] Using max model len 256\r\n",
      "WARNING 10-12 13:00:57 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \r\n",
      "WARNING 10-12 13:00:58 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 10-12 13:00:58 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='/kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233', speculative_config=None, tokenizer='/kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=256, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=/kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "WARNING 10-12 13:00:58 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 10-12 13:00:59 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 10-12 13:00:59 [cuda.py:360] Using XFormers backend.\r\n",
      "2025-10-12 13:01:04.319664: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760274064.341957      73 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1760274064.348681      73 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 10-12 13:01:07 [__init__.py:244] Automatically detected platform cuda.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:01:10 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:01:10 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:01:10 [cuda.py:360] Using XFormers backend.\r\n",
      "[W1012 13:01:20.577075030 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1012 13:01:21.933409084 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1012 13:01:30.585387374 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W1012 13:01:40.595256054 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 10-12 13:01:40 [__init__.py:1152] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:01:40 [__init__.py:1152] Found nccl from library libnccl.so.2\r\n",
      "INFO 10-12 13:01:40 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:01:40 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 10-12 13:01:41 [custom_all_reduce_utils.py:208] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 10-12 13:02:02 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:02:02 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 10-12 13:02:02 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_efa0c1cd'), local_subscribe_addr='ipc:///tmp/b382cc63-485d-45f2-b726-6d8357ef4c66', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "INFO 10-12 13:02:02 [parallel_state.py:1076] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:02:02 [parallel_state.py:1076] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\r\n",
      "INFO 10-12 13:02:02 [model_runner.py:1171] Starting to load model /kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:02:03 [model_runner.py:1171] Starting to load model /kaggle/input/exp005-8b-fulltrain-qwen3-8b-20250915235233...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:50<02:30, 50.21s/it]\r\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [01:21<01:18, 39.15s/it]\r\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:29<00:24, 24.91s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:05<00:00, 29.35s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:05<00:00, 31.43s/it]\r\n",
      "\r\n",
      "INFO 10-12 13:04:09 [default_loader.py:272] Loading weights took 125.83 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:04:09 [default_loader.py:272] Loading weights took 125.65 seconds\r\n",
      "INFO 10-12 13:04:10 [model_runner.py:1203] Model loading took 7.6369 GiB and 126.087391 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:04:10 [model_runner.py:1203] Model loading took 7.6369 GiB and 125.882219 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:04:17 [worker.py:294] model weights take 7.64GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 0.16GiB; the rest of the memory reserved for KV Cache is 6.09GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 10-12 13:04:17 [worker.py:294] model weights take 7.64GiB; non_torch_memory takes 0.12GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 4.84GiB.\r\n",
      "INFO 10-12 13:04:18 [executor_base.py:113] # cuda blocks: 4409, # CPU blocks: 3640\r\n",
      "INFO 10-12 13:04:18 [executor_base.py:118] Maximum concurrency for 256 tokens per request: 275.56x\r\n",
      "INFO 10-12 13:04:23 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 12.66 seconds\r\n",
      "[INFO] Generating (Case B)...\r\n",
      "Adding requests: 100%|███████████████████████████| 3/3 [00:00<00:00, 629.71it/s]\r\n",
      "Processed prompts: 100%|█| 3/3 [00:01<00:00,  2.09it/s, est. speed input: 273.00\r\n",
      "[INFO] Generation done in 1.4s\r\n",
      "[INFO] Saved probabilities to /kaggle/working/submission_qwen_sft_probabilities.csv\r\n",
      "[INFO] Saved submission to   /kaggle/working/submission_qwen_sft.csv\r\n",
      "[INFO] Preview probabilities:\r\n",
      "   row_id  prob_0  prob_1  prob_2  ...  prob_22  prob_23  prob_24  top_classes\r\n",
      "0   36696     0.0     0.0     0.0  ...      0.0      0.0      0.0             \r\n",
      "1   36697     0.0     0.0     0.0  ...      0.0      0.0      0.0             \r\n",
      "2   36698     0.0     0.0     0.0  ...      0.0      0.0      0.0             \r\n",
      "\r\n",
      "[3 rows x 27 columns]\r\n",
      "INFO 10-12 13:04:24 [multiproc_worker_utils.py:138] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=73)\u001b[0;0m INFO 10-12 13:04:24 [multiproc_worker_utils.py:260] Worker exiting\r\n",
      "[INFO] Done.\r\n",
      "GPU count: 2\n",
      "=== GPU 0 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "=== GPU 1 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "2025-10-12 13:04:43.388100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760274283.410517     261 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1760274283.417344     261 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Train shape: (36696, 9) with 65 target classes\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:23<00:00, 20.78s/it]\r\n",
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/gemma2-9b-it-bf16 and are newly initialized: ['score.weight']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['qalora_group_size', 'use_qalora'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\r\n",
      "  warnings.warn(\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 116.33 examples/s]\r\n",
      "Inference:   0%|                                          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\r\n",
      "  warnings.warn(\r\n",
      "Inference: 100%|██████████████████████████████████| 1/1 [00:02<00:00,  2.43s/it]\r\n",
      "GPU count: 2\n",
      "=== GPU 0 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "=== GPU 1 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "2025-10-12 13:06:32.841270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1760274392.864171     288 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1760274392.871076     288 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Train shape: (36696, 9) with 65 target classes\r\n",
      " Starting multi-GPU inference...\r\n",
      "Loading deepseek on cuda:0...\r\n",
      "Loading checkpoint shards:   0%|                          | 0/3 [00:00<?, ?it/s]Loading qwen3 on cuda:1...\r\n",
      "\r\n",
      "Loading checkpoint shards:  33%|██████            | 1/3 [00:39<01:18, 39.34s/it]\r\n",
      "Loading checkpoint shards:  67%|████████████      | 2/3 [01:17<00:38, 38.85s/it]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [01:40<00:00, 33.52s/it]\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 125.76 examples/s]\r\n",
      "deepseek: 100%|███████████████████████████████████| 1/1 [00:01<00:00,  1.11s/it]\r\n",
      " deepseek completed - saved submission and probabilities\r\n",
      "\r\n",
      "Loading checkpoint shards:  75%|█████████████▌    | 3/4 [01:54<00:38, 38.23s/it]\u001b[A\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [01:57<00:00, 29.40s/it]\r\n",
      "Map: 100%|████████████████████████████████| 3/3 [00:00<00:00, 739.17 examples/s]\r\n",
      "qwen3: 100%|██████████████████████████████████████| 1/1 [00:00<00:00,  2.95it/s]\r\n",
      " qwen3 completed - saved submission and probabilities\r\n",
      " completed in 128.75 seconds!\r\n",
      "GPU count: 2\n",
      "=== GPU 0 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n",
      "=== GPU 1 ===\n",
      "Name: Tesla T4\n",
      "Memory Allocated: 0 MB\n",
      "Memory Reserved:  0 MB\n",
      "Max Allocated:    0 MB\n",
      "Max Reserved:     0 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# vLLM (Qwen SFT, TP=2 on GPUs 0,1)\n",
    "!python /kaggle/working/vllm_qwen_sft_inference.py\n",
    "time.sleep(5)\n",
    "print_gpu_memory()\n",
    "\n",
    "# 以降は今まで通り\n",
    "!python /kaggle/working/gemma2_inference.py\n",
    "time.sleep(5)\n",
    "print_gpu_memory()\n",
    "\n",
    "!python /kaggle/working/qwen3_deepseek_inference.py\n",
    "time.sleep(5)\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8725f91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T05:24:19.923503Z",
     "iopub.status.busy": "2025-09-05T05:24:19.922686Z",
     "iopub.status.idle": "2025-09-05T05:24:19.927662Z",
     "shell.execute_reply": "2025-09-05T05:24:19.926846Z",
     "shell.execute_reply.started": "2025-09-05T05:24:19.923473Z"
    },
    "papermill": {
     "duration": 0.006133,
     "end_time": "2025-10-12T13:08:52.459428",
     "exception": false,
     "start_time": "2025-10-12T13:08:52.453295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab80a4a",
   "metadata": {
    "papermill": {
     "duration": 0.005822,
     "end_time": "2025-10-12T13:08:52.471182",
     "exception": false,
     "start_time": "2025-10-12T13:08:52.465360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd9635cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T13:08:52.484626Z",
     "iopub.status.busy": "2025-10-12T13:08:52.484352Z",
     "iopub.status.idle": "2025-10-12T13:08:52.914563Z",
     "shell.execute_reply": "2025-10-12T13:08:52.913847Z"
    },
    "papermill": {
     "duration": 0.438676,
     "end_time": "2025-10-12T13:08:52.915806",
     "exception": false,
     "start_time": "2025-10-12T13:08:52.477130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saved submission.csv\n",
      "   row_id                             Category:Misconception\n",
      "0   36696  True_Correct:NA True_Neither:NA True_Misconcep...\n",
      "1   36697  False_Misconception:WNB False_Neither:NA False...\n",
      "2   36698  True_Neither:NA True_Correct:NA True_Misconcep...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MAP - Ensemble with Disagreement Handling (Robust Version)\n",
    "# ------------------------------------------------------------\n",
    "# - 安全なCSV読み込み（top_classesのNaN→\"\"、prob_*のNaN→0.0）\n",
    "# - 行ごと抽出でもNaN安全：.get と型チェックでsplit()/float変換を保護\n",
    "# - 重み(model_weights)がNone/不足/過剰でも自動調整\n",
    "# - すべてのモデルでクラスが空でも安全（空文字を返す）\n",
    "# - デバッグプリント（最初の数行）\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def _load_prob_csv(path: str) -> pd.DataFrame:\n",
    "    \"\"\"probabilities CSV を安全に読み込んで整形\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # row_id の型をそろえる\n",
    "    if \"row_id\" not in df.columns:\n",
    "        raise KeyError(f\"'row_id' column is missing in {path}\")\n",
    "    df[\"row_id\"] = df[\"row_id\"].astype(int)\n",
    "\n",
    "    # top_classes* を安全化（NaN -> \"\"、非文字列 -> str）\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"top_classes\"):\n",
    "            df[c] = df[c].fillna(\"\").astype(str)\n",
    "\n",
    "    # prob_* を安全化（NaN -> 0.0、非数 -> 0.0）\n",
    "    for c in df.columns:\n",
    "        if c.startswith(\"prob_\"):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_class_probabilities(row: pd.Series, model_suffix: str = \"\", top_k: int = 25) -> dict:\n",
    "    \"\"\"\n",
    "    行からクラス名と確率を抽出（NaN/欠損に強い）\n",
    "    返り値: { class_name(str): prob(float), ... }\n",
    "    \"\"\"\n",
    "    classes_col = f\"top_classes{model_suffix}\"\n",
    "    raw = row.get(classes_col, \"\")\n",
    "    if isinstance(raw, float) and np.isnan(raw):\n",
    "        classes = []\n",
    "    else:\n",
    "        raw = \"\" if raw is None else str(raw)\n",
    "        classes = raw.split()[:top_k] if raw else []\n",
    "\n",
    "    class_probs = {}\n",
    "    for i, cls in enumerate(classes):\n",
    "        prob_col = f\"prob_{i}{model_suffix}\"\n",
    "        p = row.get(prob_col, 0.0)\n",
    "        try:\n",
    "            p = float(p)\n",
    "        except Exception:\n",
    "            p = 0.0\n",
    "        class_probs[cls] = p\n",
    "    return class_probs\n",
    "\n",
    "\n",
    "def _normalize_weights(n_models: int, model_weights):\n",
    "    \"\"\"重みを n_models に合わせて安全化（None→等重み、不足→後ろを1.0、過剰→切り落とし）\"\"\"\n",
    "    if model_weights is None:\n",
    "        return [1.0] * n_models\n",
    "    w = list(model_weights)\n",
    "    if len(w) < n_models:\n",
    "        w = w + [1.0] * (n_models - len(w))\n",
    "    elif len(w) > n_models:\n",
    "        w = w[:n_models]\n",
    "    return w\n",
    "\n",
    "\n",
    "def ensemble_with_disagreement_handling(prob_files, model_weights=None, top_k=3, debug=False):\n",
    "    \"\"\"\n",
    "    複数の probability CSV を row_id でマージし、重み付き合意/不一致処理で上位 top_k を返す。\n",
    "    戻り値: List[str]（各rowの \"class1 class2 class3\"）\n",
    "    \"\"\"\n",
    "    n_models = len(prob_files)\n",
    "    weights = _normalize_weights(n_models, model_weights)\n",
    "\n",
    "    # 個別読込（安全化）\n",
    "    prob_dfs = [_load_prob_csv(p) for p in prob_files]\n",
    "\n",
    "    # マージ（suffixesで列名衝突を回避）\n",
    "    merged_df = prob_dfs[0]\n",
    "    for i, df in enumerate(prob_dfs[1:], 1):\n",
    "        merged_df = pd.merge(\n",
    "            merged_df, df, on=\"row_id\", how=\"inner\",\n",
    "            suffixes=(\"\", f\"_model{i+1}\")\n",
    "        )\n",
    "\n",
    "    if debug:\n",
    "        print(\"[DEBUG] Merged columns:\", list(merged_df.columns)[:20], \"...\")\n",
    "        print(\"[DEBUG] Merged head:\\n\", merged_df.head())\n",
    "\n",
    "    final_predictions = []\n",
    "\n",
    "    for _, row in merged_df.iterrows():\n",
    "        # 各モデルから {class: prob} を抽出\n",
    "        all_class_probs = []\n",
    "        for i in range(n_models):\n",
    "            suffix = f\"_model{i+1}\" if i > 0 else \"\"\n",
    "            class_probs = extract_class_probabilities(row, suffix, top_k=25)\n",
    "            all_class_probs.append(class_probs)\n",
    "\n",
    "        # クラス集合\n",
    "        all_classes = set()\n",
    "        for class_probs in all_class_probs:\n",
    "            all_classes.update(class_probs.keys())\n",
    "\n",
    "        if not all_classes:\n",
    "            # どのモデルもクラスを出していない場合のフォールバック（空文字1つ）\n",
    "            final_predictions.append(\"\")\n",
    "            continue\n",
    "\n",
    "        # 投票・合計・最大確率（重み付き）\n",
    "        class_votes = defaultdict(int)\n",
    "        class_total_prob = defaultdict(float)\n",
    "        class_max_prob = defaultdict(float)\n",
    "\n",
    "        for i, class_probs in enumerate(all_class_probs):\n",
    "            weight = weights[i]\n",
    "            for cls, prob in class_probs.items():\n",
    "                class_votes[cls] += 1\n",
    "                class_total_prob[cls] += prob * weight\n",
    "                class_max_prob[cls] = max(class_max_prob[cls], prob * weight)\n",
    "\n",
    "        # スコア計算\n",
    "        final_scores = {}\n",
    "        n_models_float = float(n_models)\n",
    "        for cls in all_classes:\n",
    "            base = class_total_prob[cls]                 # 60%\n",
    "            agreement = class_votes[cls] / n_models_float  # 30%\n",
    "            confidence = class_max_prob[cls]             # 10%\n",
    "            final_scores[cls] = base * 0.6 + agreement * 0.3 + confidence * 0.1\n",
    "\n",
    "        # ソートして上位 top_k\n",
    "        sorted_classes = sorted(final_scores.items(), key=lambda x: -x[1])\n",
    "        top_classes = [cls for cls, _ in sorted_classes[:top_k]]\n",
    "\n",
    "        final_predictions.append(\" \".join(top_classes))\n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "\n",
    "# ====== 設定 ======\n",
    "# 単体モデル参考スコア（コメント）\n",
    "# deepseek math 7b - 0.944\n",
    "# qwen3 8b        - 0.943\n",
    "# gemma 2 9b      - 0.942\n",
    "w1, w2, w3, w4, w5 = 1.0, 1.0, 1.0, 1.0, 1.0\n",
    "\n",
    "prob_files = [\n",
    "    \"/kaggle/working/submission_gemma_prob.csv\",\n",
    "    \"/kaggle/working/submission_deepseek_probabilities.csv\",\n",
    "    \"/kaggle/working/submission_qwen3_probabilities.csv\",\n",
    "    \"/kaggle/working/submission_qwen_sft_probabilities.csv\",\n",
    "]\n",
    "\n",
    "# 読み込むファイルの数に合わせて重みを自動調整します（不足→1.0追加／過剰→切り捨て）\n",
    "predictions = ensemble_with_disagreement_handling(\n",
    "    prob_files,\n",
    "    model_weights=[w1, w2, w3, w4, w5],  # ファイル数に合わせて内部で整形\n",
    "    top_k=3,\n",
    "    debug=False,  # Trueにするとマージ後headを表示\n",
    ")\n",
    "\n",
    "# 提出CSV\n",
    "test_df = pd.read_csv(\"/kaggle/input/map-charting-student-math-misunderstandings/test.csv\")\n",
    "submission = pd.DataFrame({\n",
    "    \"row_id\": test_df.row_id.values.astype(int),\n",
    "    \"Category:Misconception\": predictions\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"[INFO] Saved submission.csv\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "isSourceIdPinned": false,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "datasetId": 2889918,
     "sourceId": 4982782,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7930680,
     "sourceId": 12559632,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7930694,
     "sourceId": 12559652,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8039184,
     "sourceId": 12719174,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8045877,
     "sourceId": 12729471,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8278373,
     "sourceId": 13071570,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8381858,
     "sourceId": 13241585,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 261755536,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 692.021807,
   "end_time": "2025-10-12T13:08:53.740170",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-12T12:57:21.718363",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
