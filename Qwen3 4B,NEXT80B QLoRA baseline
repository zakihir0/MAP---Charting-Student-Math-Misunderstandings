{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":104383,"databundleVersionId":12957508,"sourceType":"competition"},{"sourceId":267912139,"sourceType":"kernelVersion"},{"sourceId":363134,"sourceType":"modelInstanceVersion","modelInstanceId":301514,"modelId":322000},{"sourceId":576166,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":430986,"modelId":446705}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# MAP — DeepSeek-R1 LoRA (v8, timeout-safe, syntax-clean)\nThis notebook auto-switches to a fast path during Kaggle Batch runs to avoid timeouts.\n","metadata":{}},{"cell_type":"code","source":"WHEELS=\"/kaggle/input/251014-transformers-accelarate/wheels\"\n!python -m pip install --no-index --find-links=\"$WHEELS\" \\ transformers==4.57.0 accelerate==1.10.1 peft==0.17.1 evaluate==0.4.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# MAP — Qwen3 Custom SequenceClassification (Multi-Head: Category + Miscon)\n#  - Decoder-only backbone (Qwen3 4B) + Custom heads\n#  - LoRA(TaskType.SEQ_CLS), no bitsandbytes\n#  - ChatML prompt -> encoder (AutoModel) -> pooling -> 2 heads\n#  - Kaggle-friendly (FAST_SUBMIT, downsample)\n#  - evaluate 依存を排除し、compute_metrics で精度算出\n# ============================================================\n\nimport os, sys, math, time, random, json, gc\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# --- 環境設定 ---\nos.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\nos.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0\")\nRUN_TYPE = os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"Interactive\")\nFAST_SUBMIT = (RUN_TYPE == \"Batch\")\nprint(\"RUN_TYPE:\", RUN_TYPE, \"| FAST_SUBMIT:\", FAST_SUBMIT)\n\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n\n# --- 乱数シード ---\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# --- データ読み込み（必要に応じて on_bad_lines='warn' で診断しやすく） ---\ntrain_df = pd.read_csv(\"/kaggle/input/map-charting-student-math-misunderstandings/train.csv\", engine=\"python\", on_bad_lines=\"warn\")\ntest_df  = pd.read_csv(\"/kaggle/input/map-charting-student-math-misunderstandings/test.csv\",  engine=\"python\", on_bad_lines=\"warn\")\n\n# --- 正規化等ユーティリティ ---\nVALID_CATS = [\n    \"True_Correct\",\"True_Misconception\",\"True_Neither\",\n    \"False_Correct\",\"False_Misconception\",\"False_Neither\"\n]\n\ndef normalize_category(c):\n    s = str(c).strip().replace(\"-\", \"_\").replace(\" \", \"_\")\n    fixes = {\n        \"True__Correct\": \"True_Correct\",\n        \"False__Correct\": \"False_Correct\",\n        \"True_Miscon\": \"True_Misconception\",\n        \"False_Miscon\": \"False_Misconception\",\n        \"TrueNeither\": \"True_Neither\",\n        \"FalseNeither\": \"False_Neither\"\n    }\n    s = fixes.get(s, s)\n    if s in VALID_CATS:\n        return s\n    if s.startswith(\"True_\"):\n        return \"True_Correct\"\n    if s.startswith(\"False_\"):\n        return \"False_Correct\"\n    return \"True_Correct\"\n\ndef normalize_miscon(m):\n    m = str(m).strip()\n    if (m == \"\") or (m.lower() == \"nan\"):\n        return \"NA\"\n    return m\n\ndef make_target(category, miscon):\n    return f\"{normalize_category(category)}:{normalize_miscon(miscon)}\"\n\ndef chatml_prompt_dict(qtext, mc_answer, explanation, enable_thinking: bool = True):\n    sys_msg = (\n        \"You analyze student math explanations and output exactly one line: Category:Misconception. \"\n        \"Category is one of {True_Correct, True_Misconception, True_Neither, \"\n        \"False_Correct, False_Misconception, False_Neither}. Misconception is canonical or NA.\"\n    )\n    user_msg = (\n        f\"Question: {qtext}\\n\"\n        f\"SelectedOption: {mc_answer}\\n\"\n        f\"Explanation: {explanation}\\n\"\n        \"Return exactly: Category:Misconception\"\n    )\n    return {\"system\": sys_msg, \"user\": user_msg, \"enable_thinking\": enable_thinking}\n\ndef pack_for_length(qtext, mc_answer, explanation, max_chars=850):\n    qtext = qtext or \"\"; mc_answer = mc_answer or \"\"; explanation = explanation or \"\"\n    ex = explanation[: int(max_chars * 0.67)]\n    q  = qtext[: int(max_chars * 0.33)]\n    return q, mc_answer, ex\n\n# --- モデル＋トークナイザー準備 ---\nfrom transformers import AutoTokenizer, AutoModel\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nMODEL_ID = \"/kaggle/input/qwen3-next-80b/transformers/qwen3-next-80b-a3b-thinking/1\"  # Kaggle 4B base\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Backbone は AutoModel（隠れ状態を取り出して自前ヘッドへ）\nbackbone = AutoModel.from_pretrained(\n    MODEL_ID,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n    low_cpu_mem_usage=True,\n)\n# 一応 use_cache があれば無効（学習安定）\nif hasattr(backbone.config, \"use_cache\"):\n    backbone.config.use_cache = False\ntry:\n    backbone.gradient_checkpointing_enable()\nexcept Exception as e:\n    print(\"gradient_checkpointing_enable failed:\", e)\n\nprint(\"Backbone loaded. device:\", next(backbone.parameters()).device, \"| dtype:\", next(backbone.parameters()).dtype)\n\n# --- ラベル空間の構築 ---\ntrain_df[\"_Category\"] = train_df[\"Category\"].apply(normalize_category)\ntrain_df[\"_Miscon\"]   = train_df[\"Misconception\"].apply(normalize_miscon)\n\nCAT2ID = {c:i for i,c in enumerate(VALID_CATS)}\nID2CAT = {i:c for c,i in CAT2ID.items()}\nmis_list = sorted(train_df[\"_Miscon\"].fillna(\"NA\").apply(normalize_miscon).unique().tolist())\nif \"NA\" not in mis_list:\n    mis_list = [\"NA\"] + mis_list\nMIS2ID = {m:i for i,m in enumerate(mis_list)}\nID2MIS = {i:m for m,i in MIS2ID.items()}\n\nprint(f\"#Category classes: {len(CAT2ID)} | #Misconception classes: {len(MIS2ID)}\")\n\n# --- ダウンサンプリング & 例作成 ---\nfrom sklearn.model_selection import train_test_split\n\nMAX_TRAIN_SAMPLES = 6000 if not FAST_SUBMIT else 2500\ntmp = train_df.copy()\nper_class = max(1, MAX_TRAIN_SAMPLES // len(VALID_CATS))\nblocks = []\nfor c in VALID_CATS:\n    blk = tmp[tmp[\"_Category\"] == c]\n    if len(blk) > per_class:\n        blk = blk.sample(n=per_class, random_state=SEED)\n    blocks.append(blk)\nmini_df = pd.concat(blocks).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n\ndef build_examples(df, speed_fast=False):\n    rows = []\n    for _, r in df.iterrows():\n        q, a, ex = pack_for_length(\n            str(r[\"QuestionText\"]), str(r[\"MC_Answer\"]), str(r[\"StudentExplanation\"]),\n            900 if not speed_fast else 700\n        )\n        pdict = chatml_prompt_dict(q, a, ex, enable_thinking=not FAST_SUBMIT)\n        cat_id = CAT2ID[normalize_category(r[\"_Category\"])]\n        mis_id = MIS2ID[normalize_miscon(r[\"_Miscon\"])]\n        rows.append((pdict, cat_id, mis_id))\n    return rows\n\nexamples_all = build_examples(mini_df, speed_fast=FAST_SUBMIT)\ntrain_ex, val_ex = train_test_split(\n    examples_all, test_size=(0.08 if not FAST_SUBMIT else 0.05),\n    random_state=SEED, shuffle=True\n)\nprint(\"Total examples:\", len(examples_all), \"Train:\", len(train_ex), \"Val:\", len(val_ex))\n\n# --- Dataset ---\nfrom torch.utils.data import Dataset\n\nMAX_LENGTH = 384 if not FAST_SUBMIT else 320\n\nclass QwenSeqClsDataset(Dataset):\n    def __init__(self, rows, tokenizer, max_length=384):\n        self.rows = rows\n        self.tok = tokenizer\n        self.max_length = max_length\n    def __len__(self): return len(self.rows)\n    def __getitem__(self, idx):\n        pdict, cat_id, mis_id = self.rows[idx]\n        text = tokenizer.apply_chat_template(\n            [\n                {\"role\": \"system\", \"content\": pdict[\"system\"]},\n                {\"role\": \"user\",   \"content\": pdict[\"user\"]}\n            ],\n            tokenize=False,\n            add_generation_prompt=False,            # 生成ではない\n            enable_thinking=pdict.get(\"enable_thinking\", True),\n        )\n        enc = self.tok(text, max_length=self.max_length, truncation=True, padding=False, return_tensors=\"pt\")\n        return {\n            \"input_ids\": enc[\"input_ids\"][0],\n            \"attention_mask\": enc[\"attention_mask\"][0],\n            \"labels_cat\": torch.tensor(cat_id, dtype=torch.long),\n            \"labels_mis\": torch.tensor(mis_id, dtype=torch.long),\n        }\n\ntrain_ds = QwenSeqClsDataset(train_ex, tokenizer, MAX_LENGTH)\nval_ds   = QwenSeqClsDataset(val_ex,   tokenizer, MAX_LENGTH)\nprint(\"Datasets ready:\", len(train_ds), len(val_ds))\n\n# --- Custom Model（Multi-Head Sequence Classification） ---\nimport torch.nn as nn\nfrom typing import Optional, Tuple, Dict, Any\n\nclass CustomMultiHeadQwen(nn.Module):\n    \"\"\"\n    Qwen3 (decoder-only) backbone + pooling(last non-pad token) + 2 heads:\n      - Category head (num_cat)\n      - Misconception head (num_mis)\n    forward(inputs) -> dict(loss, logits_cat, logits_mis)\n    \"\"\"\n    def __init__(self, backbone, hidden_size: int, num_cat: int, num_mis: int, cls_dropout: float = 0.1):\n        super().__init__()\n        self.backbone = backbone\n        self.dropout = nn.Dropout(cls_dropout)\n        self.cat_head = nn.Linear(hidden_size, num_cat)\n        self.mis_head = nn.Linear(hidden_size, num_mis)\n\n    def _pool(self, hidden_states: torch.Tensor, attention_mask: torch.Tensor):\n        # hidden_states: [B, T, H], attention_mask: [B, T] (1=valid,0=pad)\n        # 最終非PADトークンの表現をとる\n        lengths = attention_mask.sum(dim=1)  # [B]\n        idx = (lengths - 1).clamp(min=0).unsqueeze(-1).unsqueeze(-1).expand(-1, 1, hidden_states.size(-1))  # [B,1,H]\n        gathered = hidden_states.gather(1, idx).squeeze(1)  # [B,H]\n        return gathered\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        labels_cat: Optional[torch.Tensor] = None,\n        labels_mis: Optional[torch.Tensor] = None,\n    ) -> Dict[str, Any]:\n        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=False)\n        last_hidden = outputs.last_hidden_state  # [B,T,H]\n        pooled = self._pool(last_hidden, attention_mask)\n        x = self.dropout(pooled)\n        logits_cat = self.cat_head(x)   # [B,num_cat]\n        logits_mis = self.mis_head(x)   # [B,num_mis]\n\n        loss = None\n        if (labels_cat is not None) and (labels_mis is not None):\n            loss_cat = nn.functional.cross_entropy(logits_cat, labels_cat)\n            loss_mis = nn.functional.cross_entropy(logits_mis, labels_mis)\n            loss = loss_cat + loss_mis\n\n        # Trainer が予測を拾いやすいように 'logits' にまとめも提供（連結）\n        logits = torch.cat([logits_cat, logits_mis], dim=-1)\n        return {\"loss\": loss, \"logits\": logits, \"logits_cat\": logits_cat, \"logits_mis\": logits_mis}\n\n# hidden_size 取得\nhidden_size = getattr(backbone.config, \"hidden_size\", None)\nif hidden_size is None:\n    hidden_size = getattr(backbone.config, \"n_embd\", None)\nassert hidden_size is not None, \"hidden_size を config から取得できませんでした。\"\n\ncustom_model = CustomMultiHeadQwen(\n    backbone=backbone,\n    hidden_size=hidden_size,\n    num_cat=len(CAT2ID),\n    num_mis=len(MIS2ID),\n    cls_dropout=0.1\n)\n\n# --- LoRA（SEQ_CLS）を custom_model.backbone に適用 ---\nfrom peft import LoraConfig, get_peft_model, TaskType\nlora_cfg = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    r=8 if not FAST_SUBMIT else 4,\n    lora_alpha=16 if not FAST_SUBMIT else 8,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n)\ncustom_model.backbone = get_peft_model(custom_model.backbone, lora_cfg)\ntry:\n    custom_model.backbone.print_trainable_parameters()\nexcept:\n    pass\n\n# --- Collator ---\nfrom transformers import DataCollatorWithPadding\ncollator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if torch.cuda.is_available() else None)\n\n# --- Trainer（2ヘッド対応 + compute_metrics） ---\nfrom transformers import Trainer, TrainingArguments\n\ndef split_logits_concat(logits_concat: np.ndarray, num_cat: int):\n    \"\"\"forwardで連結したlogitsを (cat, mis) に戻す\"\"\"\n    logits_cat = logits_concat[:, :num_cat]\n    logits_mis = logits_concat[:, num_cat:]\n    return logits_cat, logits_mis\n\ndef compute_metrics_fn(eval_pred):\n    \"\"\"\n    eval_pred.predictions: (N, num_cat + num_mis)\n    eval_pred.label_ids: dict/tupleが来る場合もあるので頑健に処理\n    \"\"\"\n    preds_concat, label_ids = eval_pred\n    if isinstance(preds_concat, (list, tuple)):\n        preds_concat = preds_concat[0]\n    preds_concat = np.asarray(preds_concat)\n\n    # ラベルの取り出し（Trainerが dict を返すことがある）\n    if isinstance(label_ids, dict):\n        labels_cat = np.asarray(label_ids[\"labels_cat\"])\n        labels_mis = np.asarray(label_ids[\"labels_mis\"])\n    elif isinstance(label_ids, (list, tuple)) and len(label_ids) == 2:\n        labels_cat = np.asarray(label_ids[0]); labels_mis = np.asarray(label_ids[1])\n    else:\n        # 単一配列しか来ない場合は受け取れないのでゼロにしておく\n        labels_cat = np.zeros((preds_concat.shape[0],), dtype=np.int64)\n        labels_mis = np.zeros((preds_concat.shape[0],), dtype=np.int64)\n\n    num_cat = len(CAT2ID)\n    logits_cat, logits_mis = split_logits_concat(preds_concat, num_cat)\n    pred_cat = logits_cat.argmax(-1)\n    pred_mis = logits_mis.argmax(-1)\n\n    acc_cat  = (pred_cat == labels_cat).mean().item()\n    acc_mis  = (pred_mis == labels_mis).mean().item()\n    acc_both = ((pred_cat == labels_cat) & (pred_mis == labels_mis)).mean().item()\n    return {\"acc_cat\": acc_cat, \"acc_mis\": acc_mis, \"acc_both\": acc_both}\n\nclass TwoHeadTrainer(Trainer):\n    # ラベル名を明示（'labels' 以外も集計対象に）\n    label_names = [\"labels_cat\", \"labels_mis\"]\n\n    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n        out = model(**inputs)\n        loss = out[\"loss\"]\n        return (loss, out) if return_outputs else loss\n\n    def prediction_step(\n        self, model, inputs, prediction_loss_only=False, ignore_keys=None\n    ):\n        \"\"\"\n        2ヘッドの logits を forward 側の 'logits'（連結）として返し、\n        labels は dict のまま Trainer に渡す。\n        \"\"\"\n        model.eval()\n        with torch.no_grad():\n            outputs = model(**{k:v for k,v in inputs.items() if k in [\"input_ids\",\"attention_mask\",\"labels_cat\",\"labels_mis\"]})\n            loss = outputs.get(\"loss\", None)\n            logits = outputs[\"logits\"]  # (B, num_cat+num_mis)\n        # labels は dict のまま返し、Trainer 側でそのまま metrics に渡す\n        labels = {k: v for k, v in inputs.items() if k in self.label_names}\n        if prediction_loss_only:\n            return (loss, None, None)\n        return (loss, logits, labels)\n\n# ラッパーデータセット（Trainer が未知キーに難しいのでラベル名はそのまま）\nclass TrainWrap(torch.utils.data.Dataset):\n    def __init__(self, base): self.base=base\n    def __len__(self): return len(self.base)\n    def __getitem__(self, i): return self.base[i]\n\ntrain_wrap = TrainWrap(train_ds)\nval_wrap   = TrainWrap(val_ds)\n\n# --- 学習設定 ---\nBATCH_SIZE = 4 if not FAST_SUBMIT else 4\nGRAD_ACCUM = 4 if not FAST_SUBMIT else 4\nLR = 2e-5\nEPOCHS = 2 if not FAST_SUBMIT else 1\n\nargs = TrainingArguments(\n    output_dir=\"out_custom\",\n    overwrite_output_dir=True,\n    bf16=False,\n    fp16=torch.cuda.is_available(),\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRAD_ACCUM,\n    learning_rate=LR,\n    num_train_epochs=EPOCHS,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.05,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    logging_steps=50,\n    report_to=[],\n)\n\ntrainer = TwoHeadTrainer(\n    model=custom_model,\n    args=args,\n    train_dataset=train_wrap,\n    eval_dataset=val_wrap,\n    data_collator=collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics_fn,\n)\n\nprint(\"\\n== Train (Custom Multi-Head) ==\")\nt0 = time.time()\ntrainer.train()\nprint(\"Train seconds:\", time.time() - t0)\n\ntrainer.save_model(\"out_custom/final\")\ntokenizer.save_pretrained(\"out_custom/final\")\n\n# --- 推論（カテゴリ argmax + 誤概念 Top-k） ---\nfrom torch.nn.functional import softmax\n\n@torch.no_grad()\ndef encode_inputs_from_pdict(pdict, max_len=384):\n    text = tokenizer.apply_chat_template(\n        [\n            {\"role\": \"system\", \"content\": pdict[\"system\"]},\n            {\"role\": \"user\",   \"content\": pdict[\"user\"]},\n        ],\n        tokenize=False,\n        add_generation_prompt=False,\n        enable_thinking=pdict.get(\"enable_thinking\", True),\n    )\n    enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_len, padding=False)\n    if torch.cuda.is_available():\n        enc = {k:v.to(next(custom_model.parameters()).device) for k,v in enc.items()}\n    return enc\n\n@torch.no_grad()\ndef predict_cat_and_mis(enc, topk=3):\n    custom_model.eval()\n    out = custom_model(**enc)\n    cat_idx = out[\"logits_cat\"].argmax(dim=-1).item()\n    probs_mis = softmax(out[\"logits_mis\"], dim=-1)[0]\n    k = min(topk, probs_mis.size(-1))\n    vals, idx = torch.topk(probs_mis, k)\n    cat_str = ID2CAT[int(cat_idx)]\n    mis_list = [ID2MIS[int(i)] for i in idx.tolist()]\n    return cat_str, mis_list\n\nrows = []\nk_pred = 3\nfor _, r in test_df.iterrows():\n    q = str(r[\"QuestionText\"]); a = str(r[\"MC_Answer\"]); ex = str(r[\"StudentExplanation\"])\n    q_trim = q[:300 if not FAST_SUBMIT else 140]\n    ex_trim = ex[:900 if not FAST_SUBMIT else 700]\n    pdict = chatml_prompt_dict(q_trim, a, ex_trim, enable_thinking=not FAST_SUBMIT)\n\n    enc = encode_inputs_from_pdict(pdict, max_len=(384 if not FAST_SUBMIT else 320))\n    cat_str, mis_topk = predict_cat_and_mis(enc, topk=k_pred)\n    pairs = [f\"{cat_str}:{m if m else 'NA'}\" for m in mis_topk]\n    rows.append((r[\"row_id\"], \" \".join(pairs)))\n\nsub = pd.DataFrame(rows, columns=[\"row_id\", \"Category:Misconception\"])\nout_path = \"/kaggle/working/submission.csv\"\nsub.to_csv(out_path, index=False)\nprint(\"Saved:\", out_path)\ndisplay(sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-23T10:28:22.874937Z","iopub.execute_input":"2025-09-23T10:28:22.875304Z","iopub.status.idle":"2025-09-23T10:28:22.879944Z","shell.execute_reply.started":"2025-09-23T10:28:22.875273Z","shell.execute_reply":"2025-09-23T10:28:22.879293Z"}},"outputs":[],"execution_count":null}]}